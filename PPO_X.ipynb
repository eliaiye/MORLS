{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c996157-6ada-4abc-ac96-54a2af63a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7df59c2-9843-48a4-b860-ed0a0c0368ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimEnv(gym.Env):\n",
    "    def __init__(self, b=10, c=0.5, e=3, y0=2, threshold=1, penalty=2, timeout=100):\n",
    "        '''\n",
    "        action_space: (y, delta_t)\n",
    "        observation_space: (x)\n",
    "        \n",
    "        Input\n",
    "        threshold: when x is lower than this threshold, patient die\n",
    "        penalty: this parameter adds cost to frequent visits\n",
    "        '''\n",
    "        super(SimEnv, self).__init__()\n",
    "        \n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.e = e\n",
    "        self.y0 = y0\n",
    "        self.threshold = threshold\n",
    "        self.penalty = penalty\n",
    "            \n",
    "        self.action_space = gym.spaces.Box(low=0,high=float(\"inf\"),shape=(2,),dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=-float(\"inf\"), high=float(\"inf\"), shape=(1,), dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.timeout = timeout\n",
    "        self.steps_elapsed = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(self.threshold, 15, 1) # numpy array (1,)\n",
    "        self.steps_elapsed=0\n",
    "        return(self.state)\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        delta_t=action[1]\n",
    "        y=action[0]\n",
    "        next_obs = self.xfunc(x=self.state, delta_t=delta_t, y=action[0], b=self.b,c= self.c, e=self.e, y0=self.y0)\n",
    "        \n",
    "        if next_obs > self.threshold:\n",
    "            reward = delta_t - self.penalty\n",
    "        elif (-1/self.c*np.log((self.threshold-self.state[0]+self.b-self.e*(2*y*self.y0-y**2))/self.b) < 0):\n",
    "            reward = -self.penalty \n",
    "        else:\n",
    "            reward = -1/self.c*np.log((self.threshold-self.state[0]+self.b-self.e*(2*y*self.y0-y**2))/self.b) - self.penalty\n",
    "            \n",
    "        self.state = next_obs \n",
    "        self.steps_elapsed+=1\n",
    "        \n",
    "        return(np.array([self.state]).astype(np.float32), reward, next_obs[0] <= self.threshold or self.steps_elapsed > self.timeout , {\"x\": self.state, \"y\": action[0], \"delta_t\": action[1], \"dead\": next_obs <= self.threshold})\n",
    "    \n",
    "    def xfunc(self, x, delta_t, y, b, c, e, y0):\n",
    "        '''\n",
    "        Input\n",
    "        x: last obs/state\n",
    "        delta_t: time interval btw visits\n",
    "        y: dosage\n",
    "        b, c, e, y0: parameters\n",
    "        Output\n",
    "        current obs/state\n",
    "        '''\n",
    "        return(x-b+b*np.exp(-c*delta_t)+e*(2*y*y0-y**2)) \n",
    "    \n",
    "    def render(self, mode=None):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7722077-bd93-4bae-8cd0-9ea327b62c4c",
   "metadata": {},
   "source": [
    "Actor Critic Policy:\n",
    "- Two-layer MLP, 64 neurons, activation tanh, output Gaussian mean\n",
    "- Log standard deviation (same dimension as action)\n",
    "- Policy: $\\mathcal{N}(\\mbox{mean}=\\mbox{NeuralNet}(s;{W_i, b_i}^L_{i=1}), \\mbox{stdev}=e^r)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96843a09-9b19-4841-996f-69033ea7cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_neurons=64, activation=nn.Tanh, distribution = torch.distributions.multivariate_normal.MultivariateNormal):\n",
    "        '''\n",
    "        input dim = obs dim\n",
    "        output dim = action dim\n",
    "        '''\n",
    "        super(ActorCriticPolicy, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_neurons = n_neurons\n",
    "        self.distribution = distribution\n",
    "\n",
    "        # Policy Network\n",
    "        self.h0 = nn.Linear(input_dim, n_neurons)\n",
    "        self.h0_act = activation()\n",
    "        self.h1 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.h1_act = activation()\n",
    "        self.output_layer = nn.Linear(n_neurons, output_dim)\n",
    "\n",
    "        # Value Network\n",
    "        self.h0v = nn.Linear(input_dim, n_neurons)\n",
    "        self.h0_actv = activation()\n",
    "        self.h1v = nn.Linear(n_neurons, n_neurons)\n",
    "        self.h1_actv = activation()\n",
    "        self.value_head = nn.Linear(n_neurons, 1)\n",
    "\n",
    "        self.variance = torch.nn.Parameter(torch.tensor([0.0]*self.output_dim), requires_grad = True)\n",
    "\n",
    "        #self.mean_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, obs, action=None):\n",
    "        '''\n",
    "        obs: tensor.size([input_dim])\n",
    "        '''\n",
    "        # Policy Forward: obs->action\n",
    "        x = self.h0(obs)\n",
    "        x = self.h0_act(x)\n",
    "        x = self.h1(x)\n",
    "        x = self.h1_act(x)\n",
    "        action_mean = self.output_layer(x) #tensor.size([#obs,act_dim])\n",
    "\n",
    "        action_variance = torch.exp(self.variance)\n",
    "        action_dist = self.distribution(action_mean, torch.diag_embed(action_variance)) #tensor.size([#obs, obs_dim, obs_dim])\n",
    "\n",
    "        if action is None:\n",
    "            action = action_dist.sample() #tensor.size([#obs, act_dim])\n",
    "        \n",
    "        logprob = action_dist.log_prob(action) #tensor.size([#obs])\n",
    "        \n",
    "        # Value Forward: obs-> state value(V)\n",
    "        x = self.h0v(obs)\n",
    "        x = self.h0_actv(x)\n",
    "        x = self.h1v(x)\n",
    "        x = self.h1_actv(x)\n",
    "        value = self.value_head(x)\n",
    "        value = torch.squeeze(value) #tensor.size([#obs,1]) - > tensor.size([#obs])\n",
    "\n",
    "        return action, logprob, value # action: tensor.size([#obs, act_dim]), logprob: tensor.size([#obs]), value: tensor.size([#obs])\n",
    "        # Question: Do we need the entropy of the distribution?\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00807b30-0714-4a70-ba9b-5b4e153573b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_X():\n",
    "    def __init__(self, input_dim, output_dim, network=ActorCriticPolicy):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.policy = network(input_dim, output_dim)\n",
    "        \n",
    "        self.total_obs = []\n",
    "        self.total_act = []\n",
    "        self.total_rew = []\n",
    "        self.total_ep_rew=[]\n",
    "        self.total_ep_len=[]\n",
    "        \n",
    "    def forward(self, observation, action = None):\n",
    "        return self.policy.forward(observation, action=action)\n",
    "    \n",
    "    def rollout(self, env, timesteps, gamma, lamb): # timesteps << episode length\n",
    "        '''\n",
    "        Run policy pi_old in environment for T timesteps\n",
    "        Compute advantage estimates A_1,...,A_T\n",
    "        '''\n",
    "        obs_trajectory = []\n",
    "        action_trajectory = []\n",
    "        logprob_trajectory = []\n",
    "        value_trajectory = []\n",
    "        reward_trajectory = []\n",
    "        done_trajectory = []\n",
    "        \n",
    "        done = True\n",
    "        for i in range(timesteps):\n",
    "            if done:\n",
    "                obs = torch.unsqueeze(torch.tensor(env.reset()).float(),0) # numpy array shape (obs_dim, ) \n",
    "                                                                   # -> tensor.size([obs_dim])\n",
    "                                                                   # -> tensor.size([1, obs_dim])\n",
    "                done = False\n",
    "            \n",
    "            action, logprob, value = self.forward(observation = obs) # action: tensor.size([1, act_dim]), logprob: tensor.size([1]), value: tensor.size([1])\n",
    "            \n",
    "            action = torch.squeeze(action) # tensor([act_dim])\n",
    "            logprob = torch.squeeze(logprob) # tensor(1)\n",
    "            value = torch.squeeze(value) # tensor(1)\n",
    "            \n",
    "            action_trajectory.append(action)\n",
    "            logprob_trajectory.append(logprob)\n",
    "            value_trajectory.append(value)\n",
    "            obs_trajectory.append(torch.squeeze(obs))\n",
    "            done_trajectory.append(done)\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action.numpy().astype(float)) # step() takes np array shape(act_dim,)\n",
    "            obs = torch.unsqueeze(torch.tensor(obs).float(),0)\n",
    "            reward = torch.tensor(reward).float() # tensor.size([])\n",
    "            #print(\"reward\", reward, \"done\", done)\n",
    "            \n",
    "            reward_trajectory.append(reward)\n",
    "        # All trajectories are lists of squeezed tensors, done_trajectory is a list of bool\n",
    "        # List->stacked tensor\n",
    "        obs_trajectory = torch.stack(obs_trajectory) # tensor.size([#obs, obs_dim])\n",
    "        action_trajectory = torch.stack(action_trajectory)\n",
    "        logprob_trajectory = torch.stack(logprob_trajectory)# tensor.size([#obs])\n",
    "        value_trajectory = torch.stack(value_trajectory)\n",
    "        reward_trajectory = torch.stack(reward_trajectory)  \n",
    "        \n",
    "        done_trajectory = np.asarray(done_trajectory, dtype=np.bool_)\n",
    "        \n",
    "        _, __, last_value = self.forward(obs)\n",
    "        \n",
    "        adv_trajectory = torch.zeros_like(reward_trajectory).float()\n",
    "        delta_trajectory = torch.zeros_like(reward_trajectory).float()\n",
    "        \n",
    "        for t in reversed(range(timesteps)): #T-1 -> 0\n",
    "            \n",
    "            if done_trajectory[t]:\n",
    "                delta_trajectory[t] = reward_trajectory[t]-value_trajectory[t]\n",
    "                adv_trajectory[t]=delta_trajectory[t]\n",
    "            \n",
    "            else:\n",
    "                if t == timesteps-1:\n",
    "                    delta_trajectory[t] = reward_trajectory[t]-value_trajectory[t]\n",
    "                    adv_trajectory[t]=delta_trajectory[t]\n",
    "                else:\n",
    "                    delta_trajectory[t] = reward_trajectory[t]+gamma*value_trajectory[t+1]-value_trajectory[t]\n",
    "                    adv_trajectory[t]=delta_trajectory[t]+gamma*lamb*adv_trajectory[t+1]\n",
    "            \n",
    "#             if t == timesteps-1:\n",
    "#                 adv_trajectory[t]=delta_trajectory[t]\n",
    "                \n",
    "                \n",
    "#             else:\n",
    "#                 adv_trajectory[t]=delta_trajectory[t]+gamma*lamb*adv_trajectory[t+1] #recursion\n",
    "        #print(\"in rollout:\",obs_trajectory.size())\n",
    "        return(obs_trajectory, action_trajectory, logprob_trajectory, adv_trajectory)\n",
    "    \n",
    "    def loss(self, epsilon, obs_traj, action_traj, logprob_traj, adv_traj, current_logprob):\n",
    "        # ratio\n",
    "        ratio = torch.exp(current_logprob-logprob_traj) # torch.size([#obs])\n",
    "        \n",
    "        # loss\n",
    "        l1 = ratio*adv_traj # torch.size([#obs])\n",
    "        l2 = torch.clamp(ratio, 1.-epsilon, 1.+epsilon) # torch.size([#obs])\n",
    "        # Minimize negative L\n",
    "        PGloss = -torch.mean(torch.min(l1, l2))# torch.size([#obs])\n",
    "            \n",
    "        return PGloss # torch.size([])\n",
    "    \n",
    "    def train_step(self, epsilon, obs_traj, action_traj, logprob_traj, adv_traj):\n",
    "        \n",
    "        #Question: adv needs normalization?\n",
    "        \n",
    "        self.policy.train() # module train mode\n",
    "        with torch.set_grad_enabled(True):\n",
    "            action, logprob, value = self.forward(obs_traj, action=action_traj)\n",
    "            \n",
    "            PGloss = self.loss(epsilon, obs_traj, action_traj, logprob_traj, adv_traj,logprob)\n",
    "            \n",
    "            PGloss.backward()\n",
    "            \n",
    "            return(PGloss)\n",
    "    \n",
    "    def train(self, env, total_timesteps, timesteps=1024, k_epoch=4, num_batch=4, n_actor=1, \n",
    "              epsilon=0.2, gamma = 0.99, lamb=0.95, lr = 0.00027, optimizer = torch.optim.Adam, evaluate=True):\n",
    "        \n",
    "        n_updates = total_timesteps//timesteps # number of iterations\n",
    "        \n",
    "        self.policy_optimizer=optimizer(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        for i in tqdm(range(n_updates)):\n",
    "            actor=0\n",
    "            obs_trajectory, action_trajectory, logprob_trajectory, adv_trajectory = self.rollout(env, timesteps, gamma, lamb)\n",
    "            for actor in range(n_actor-1):\n",
    "                more_obs_traj, more_action_traj, more_logprob_traj, more_adv_traj = self.rollout(env, timesteps, gamma, lamb)\n",
    "                obs_trajectory = torch.cat((obs_trajectory, more_obs_traj),0)\n",
    "                action_trajectory = torch.cat((action_trajectory, more_action_traj),0)\n",
    "                logprob_trajectory = torch.cat((logprob_trajectory, more_logprob_traj),0)\n",
    "                adv_trajectory = torch.cat((adv_trajectory, more_adv_traj),0)\n",
    "            \n",
    "            indices = np.arange(n_actor*timesteps) # [0, 1, ..., n_actor*timesteps-1]\n",
    "            \n",
    "            for k in range(k_epoch):\n",
    "                np.random.shuffle(indices)\n",
    "                batch_size = n_actor*timesteps//num_batch\n",
    "                \n",
    "                if(timesteps%num_batch):\n",
    "                    batch_size += 1\n",
    "                \n",
    "                for b in range(num_batch):\n",
    "                    #reset gradient\n",
    "                    self.policy.zero_grad()\n",
    "                    \n",
    "                    if b != num_batch-1: # all batches except the last one\n",
    "                        batch_indices = indices[b*batch_size:(b+1)*batch_size]\n",
    "                    else:\n",
    "                        batch_indices = indices[b*batch_size:] # last batch\n",
    "                \n",
    "                    batch=[tensor[batch_indices] for tensor in (torch.unsqueeze(obs_trajectory,1), action_trajectory, logprob_trajectory, adv_trajectory)]\n",
    "                    #print(\"in train\", batch[0])\n",
    "                    PGloss = self.train_step(epsilon, *batch) \n",
    "                    \n",
    "                    #Question: Do we need to clip gradient?\n",
    "                    \n",
    "                    self.policy_optimizer.step()\n",
    "            if evaluate:\n",
    "                eval_policy(env)\n",
    "                    \n",
    "        def eval_policy(self, env, n_ep=5, timeout=None):\n",
    "            if timeout==None:\n",
    "                timeout = env.timeout\n",
    "                \n",
    "            for i_episode in range(n_ep):\n",
    "                obs = []\n",
    "                act = []\n",
    "                rew = []\n",
    "                ep_reward_ls = []\n",
    "                ep_len_ls=[]\n",
    "                ep_reward = 0\n",
    "                ep_len=0\n",
    "                observation = self.env.reset()\n",
    "                obs.append(observation)\n",
    "                for t in range(timeout):\n",
    "                    action = self.act_sample()\n",
    "                    observation, reward, done, info = self.env.step(action)\n",
    "                    #print(\"reward:\", reward)\n",
    "                    ep_reward += reward\n",
    "                    ep_len +=1\n",
    "                    rew.append(reward)\n",
    "                    act.append(action)\n",
    "                    obs.append(observation)\n",
    "\n",
    "                    if done or t == 99:\n",
    "                        #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                        #print(\"Survival time: {}\".format(ep_reward+env.penalty*(t+1)))\n",
    "                        break\n",
    "                        \n",
    "                ep_reward_ls.append(ep_reward) \n",
    "                ep_len_ls.append(ep_len)\n",
    "                self.total_rew.append(rew)\n",
    "                self.total_act.append(act)\n",
    "                self.total_obs.append(obs)\n",
    "\n",
    "            self.total_ep_rew.append(np.mean(ep_reward))\n",
    "            self.total_ep_len.append(np.mean(ep_len))\n",
    "         \n",
    "    def eval(self, obs):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            obs = torch.unsqueeze(torch.tensor(obs).float(),0)\n",
    "            action, _, __=self.forward(obs)\n",
    "        return(torch.squeeze(action))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf671272-3524-4dc2-8652-67cc837f2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env =SimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67398712-93e0-40b9-acfb-8cb7b72ea466",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=PPO_X(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ad028-2a98-4ad6-bf67-5766282d0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.train(env=env, total_timesteps=10000, timesteps=100) # 100 iterations, 1 actor, 4 epochs, batch size=100/4=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3d856-c081-4548-9175-23c27871fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(torch.tensor([1.,2.]), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b5ffd-366b-46e4-a40c-7973be8dc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([torch.tensor(1.),torch.tensor(2.)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6952f-e956-4c88-a1bd-132493b1fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.tensor([1.,2.,3.]),torch.tensor([1.,2.,3.])),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9e375-2211-4f98-b665-d5fa3db62dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i in range(10):\n",
    "    state=np.random.uniform(1, 15, 4)\n",
    "    ls.append(torch.squeeze(torch.unsqueeze(torch.tensor(state),0)))\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc6671-c30c-49ce-b219-7a7dc220207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577186e-ca01-4e68-adfc-bf3e62708449",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs2=np.array([[1.,2.,3.,4.,5.], [5.,4.,3.,2.,1.]])\n",
    "print(np.shape(obs2))\n",
    "obs2 = torch.tensor(obs2)\n",
    "obs2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34906e56-9b01-457b-9170-1d2acfb02b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unsqueeze(obs2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf934a8c-edf4-41e0-b6c5-01740c9a493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b77e7-32aa-48ec-a7bb-1040508acce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e2c60-80f6-4adc-927d-c3b2f4156b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PPO_X(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212c7e8-27b6-4c12-a2f4-09b080bcb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603ad8e-2c72-47bf-9fca-8141359d7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac= ActorCriticPolicy(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756e2a9-faf4-4ba7-94cd-2dbd39bb3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.forward(torch.tensor([[1.],[2.],[3.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b9161-6c4f-46a6-9f08-6468a1d0949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac(torch.tensor([[1.,2.,3.,4.,5.], [5.,4.,3.,2.,1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614de09-0f20-484b-85b1-29aa5f654445",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=5\n",
    "n_neurons=64\n",
    "output_dim=4\n",
    "obs=torch.tensor([[1.,2.,3.,4.,5.], [5.,4.,3.,2.,1.]])\n",
    "\n",
    "h0 = nn.Linear(input_dim, n_neurons)\n",
    "h0_act = nn.Tanh()\n",
    "h1 = nn.Linear(n_neurons, n_neurons)\n",
    "h1_act = nn.Tanh()\n",
    "output_layer = nn.Linear(n_neurons, output_dim)\n",
    "\n",
    "x = h0(obs)\n",
    "x = h0_act(x)\n",
    "x = h1(x)\n",
    "x = h1_act(x)\n",
    "action_logit = output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b005d24-16a2-4808-8fc3-ca6a1292e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_logit.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2be3f-ba29-4425-ad88-aca4192bb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=5\n",
    "n_neurons=64\n",
    "output_dim=4\n",
    "obs=torch.tensor([[1.,2.,3.,4.,5.], [5.,4.,3.,2.,1.]])\n",
    "\n",
    "h0v = nn.Linear(input_dim, n_neurons)\n",
    "h0_actv = nn.Tanh()\n",
    "h1v = nn.Linear(n_neurons, n_neurons)\n",
    "h1_actv = nn.Tanh()\n",
    "value_head = nn.Linear(n_neurons, 1)\n",
    "\n",
    "x = h0v(obs)\n",
    "x = h0_actv(x)\n",
    "x = h1v(x)\n",
    "x = h1_actv(x)\n",
    "value = value_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81d1ab-d283-4924-b656-3f45b9d79c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2753ef-4583-48df-aaa2-f10bceeb6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = torch.squeeze(value)\n",
    "value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbbf63-64b6-4b4c-ae34-f369638ae475",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag_embed(torch.exp(torch.tensor([0.0]*output_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c61621-7e41-4968-a350-3a2b4bbfe745",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.multivariate_normal.MultivariateNormal(action_logit, torch.diag_embed(torch.exp(torch.tensor([0.0]*output_dim))))\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507da4a-589d-4dc7-b537-28855b81f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.log_prob(torch.tensor([[1,2,3,4],[2,3,4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac593766-08aa-402c-a615-226a52817c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.log_prob(torch.tensor([[1,2,3,4],[2,3,4,5]])).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79310f5d-82a0-4831-aecf-2b9011d1f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2,3,4]]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a549d-10e1-4cd0-b5ab-f40b1b2da492",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.squeeze(torch.tensor([[1,2,3,4]])).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df5d6a-df6f-493f-9f7f-c1d8436d1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco_py\n",
    "env = gym.make('HalfCheetah-v2')\n",
    "env2 = SimEnv()\n",
    "P2=PPO2(17,6)\n",
    "PX=PPO_X(17,6)\n",
    "P2.train(env=env2) #train policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e247c39-496a-498e-a038-1157b643ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PX.train(env=env, total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ae216-c17f-415e-82f8-813ae6d12399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
