{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "from pathlib import Path\n",
    "curr_path = str(Path().absolute())\n",
    "parent_path = str(Path().absolute().parent)\n",
    "sys.path.append(parent_path) # add current terminal path to sys.path\n",
    "from MORLS.env.SimEnv import SimEnv\n",
    "\n",
    "# benchmark (continuous cartpole) for fun\n",
    "import mbrl.env.cartpole_continuous as cartpole_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "\n",
    "from sac.replay_memory import ReplayMemory\n",
    "from sac.sac import SAC\n",
    "from model import EnsembleDynamicsModel\n",
    "from predict_env import PredictEnv\n",
    "from sample_env import EnvSampler\n",
    "from tf_models.constructor import construct_model, format_samples_for_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, env_sampler, predict_env, agent, env_pool, model_pool):\n",
    "    total_step = 0\n",
    "    reward_sum = 0\n",
    "    rollout_length = 1\n",
    "    exploration_before_start(args, env_sampler, env_pool, agent)\n",
    "\n",
    "    for epoch_step in range(args.num_epoch):\n",
    "        print('epoch_step: {}'.format(epoch_step))\n",
    "        start_step = total_step\n",
    "        train_policy_steps = 0\n",
    "        for i in count():\n",
    "            if i%50==0: print('i:{}'.format(i))\n",
    "            \n",
    "            cur_step = total_step - start_step\n",
    "            if cur_step%50==0:print('cur step:{}'.format(cur_step))\n",
    "            #if cur_step >= start_step + args.epoch_length and len(env_pool) > args.min_pool_size:\n",
    "            if cur_step >=args.epoch_length and len(env_pool) > args.min_pool_size:\n",
    "                break\n",
    "\n",
    "            if cur_step > 0 and cur_step % args.model_train_freq == 0 and args.real_ratio < 1.0:\n",
    "                train_predict_model(args, env_pool, predict_env)\n",
    "\n",
    "                new_rollout_length = set_rollout_length(args, epoch_step)\n",
    "                if rollout_length != new_rollout_length:\n",
    "                    rollout_length = new_rollout_length\n",
    "                    model_pool = resize_model_pool(args, rollout_length, model_pool)\n",
    "\n",
    "                rollout_model(args, predict_env, agent, model_pool, env_pool, rollout_length)\n",
    "\n",
    "            cur_state, action, next_state, reward, done, info = env_sampler.sample(agent)\n",
    "            env_pool.push(cur_state, action, reward, next_state, done)\n",
    "\n",
    "            if len(env_pool) > args.min_pool_size:\n",
    "                train_policy_steps += train_policy_repeats(args, total_step, train_policy_steps, cur_step, env_pool, model_pool, agent)\n",
    "\n",
    "            total_step += 1\n",
    "\n",
    "            if total_step % 100 == 0:\n",
    "                print('total step:{}'.format(total_step))\n",
    "                '''\n",
    "                avg_reward_len = min(len(env_sampler.path_rewards), 5)\n",
    "                avg_reward = sum(env_sampler.path_rewards[-avg_reward_len:]) / avg_reward_len\n",
    "                logging.info(\"Step Reward: \" + str(total_step) + \" \" + str(env_sampler.path_rewards[-1]) + \" \" + str(avg_reward))\n",
    "                print(total_step, env_sampler.path_rewards[-1], avg_reward)\n",
    "                '''\n",
    "                env_sampler.current_state = None\n",
    "                sum_reward = 0\n",
    "                done = False\n",
    "                while not done:\n",
    "                    cur_state, action, next_state, reward, done, info = env_sampler.sample(agent, eval_t=True)\n",
    "                    sum_reward += reward\n",
    "                # logger.record_tabular(\"total_step\", total_step)\n",
    "                # logger.record_tabular(\"sum_reward\", sum_reward)\n",
    "                # logger.dump_tabular()\n",
    "                logging.info(\"Step Reward: \" + str(total_step) + \" \" + str(sum_reward))\n",
    "                print(total_step, sum_reward)\n",
    "                \n",
    "\n",
    "def exploration_before_start(args, env_sampler, env_pool, agent):\n",
    "    for i in range(args.init_exploration_steps):\n",
    "        #print(\"exploration i:{}\".format(i))\n",
    "        cur_state, action, next_state, reward, done, info = env_sampler.sample(agent)\n",
    "        #print(\"cur_state:{}\".format(cur_state))\n",
    "        #print(\"action:{}\".format(action))\n",
    "        #print(\"next_state:{}\".format(next_state))\n",
    "        #print(\"reward:{}\".format(reward))\n",
    "        env_pool.push(cur_state, action, reward, next_state, done)\n",
    "    print(\"done exploration before starting\")\n",
    "\n",
    "def set_rollout_length(args, epoch_step):\n",
    "    rollout_length = (min(max(args.rollout_min_length + (epoch_step - args.rollout_min_epoch)\n",
    "                              / (args.rollout_max_epoch - args.rollout_min_epoch) * (args.rollout_max_length - args.rollout_min_length),\n",
    "                              args.rollout_min_length), args.rollout_max_length))\n",
    "    return int(rollout_length)\n",
    "\n",
    "\n",
    "def train_predict_model(args, env_pool, predict_env):\n",
    "    print(\"train predict model\")\n",
    "    # Get all samples from environment\n",
    "    state, action, reward, next_state, done = env_pool.sample(len(env_pool))\n",
    "    delta_state = next_state - state\n",
    "    inputs = np.concatenate((state, action), axis=-1)\n",
    "    labels = np.concatenate((np.reshape(reward, (reward.shape[0], -1)), delta_state), axis=-1)\n",
    "\n",
    "    predict_env.model.train(inputs, labels, batch_size=256, holdout_ratio=0.2)\n",
    "\n",
    "\n",
    "def resize_model_pool(args, rollout_length, model_pool):\n",
    "    rollouts_per_epoch = args.rollout_batch_size * args.epoch_length / args.model_train_freq\n",
    "    model_steps_per_epoch = int(rollout_length * rollouts_per_epoch)\n",
    "    new_pool_size = args.model_retain_epochs * model_steps_per_epoch\n",
    "\n",
    "    sample_all = model_pool.return_all()\n",
    "    new_model_pool = ReplayMemory(new_pool_size)\n",
    "    new_model_pool.push_batch(sample_all)\n",
    "\n",
    "    return new_model_pool\n",
    "\n",
    "\n",
    "def rollout_model(args, predict_env, agent, model_pool, env_pool, rollout_length):\n",
    "    state, action, reward, next_state, done = env_pool.sample_all_batch(args.rollout_batch_size)\n",
    "    for i in range(rollout_length):\n",
    "        # TODO: Get a batch of actions\n",
    "        action = agent.select_action(state)\n",
    "        next_states, rewards, terminals, info = predict_env.step(state, action)\n",
    "        # TODO: Push a batch of samples\n",
    "        model_pool.push_batch([(state[j], action[j], rewards[j], next_states[j], terminals[j]) for j in range(state.shape[0])])\n",
    "        nonterm_mask = ~terminals.squeeze(-1)\n",
    "        if nonterm_mask.sum() == 0:\n",
    "            break\n",
    "        state = next_states[nonterm_mask]\n",
    "\n",
    "\n",
    "def train_policy_repeats(args, total_step, train_step, cur_step, env_pool, model_pool, agent):\n",
    "    #print(\"train policy repeats\")\n",
    "    if total_step % args.train_every_n_steps > 0:\n",
    "        return 0\n",
    "\n",
    "    if train_step > args.max_train_repeat_per_step * total_step:\n",
    "        return 0\n",
    "\n",
    "    for i in range(args.num_train_repeat):\n",
    "        env_batch_size = int(args.policy_train_batch_size * args.real_ratio)\n",
    "        model_batch_size = args.policy_train_batch_size - env_batch_size\n",
    "\n",
    "        env_state, env_action, env_reward, env_next_state, env_done = env_pool.sample(int(env_batch_size))\n",
    "\n",
    "        if model_batch_size > 0 and len(model_pool) > 0:\n",
    "            model_state, model_action, model_reward, model_next_state, model_done = model_pool.sample_all_batch(int(model_batch_size))\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = np.concatenate((env_state, model_state), axis=0), \\\n",
    "                                                                                    np.concatenate((env_action, model_action),\n",
    "                                                                                                   axis=0), np.concatenate(\n",
    "                (np.reshape(env_reward, (env_reward.shape[0], -1)), model_reward), axis=0), \\\n",
    "                                                                                    np.concatenate((env_next_state, model_next_state),\n",
    "                                                                                                   axis=0), np.concatenate(\n",
    "                (np.reshape(env_done, (env_done.shape[0], -1)), model_done), axis=0)\n",
    "        else:\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = env_state, env_action, env_reward, env_next_state, env_done\n",
    "\n",
    "        batch_reward, batch_done = np.squeeze(batch_reward), np.squeeze(batch_done)\n",
    "        batch_done = (~batch_done).astype(int)\n",
    "        agent.update_parameters((batch_state, batch_action, batch_reward, batch_next_state, batch_done), args.policy_train_batch_size, i)\n",
    "\n",
    "    return args.num_train_repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args=None):\n",
    "    if args is None:\n",
    "        args = readParser()\n",
    "\n",
    "\n",
    "    \n",
    "    env=SimEnv()\n",
    "    # Set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    env.seed(args.seed)\n",
    "\n",
    "    # Intial agent\n",
    "    agent = SAC(env.observation_space.shape[0], env.action_space, args)\n",
    "\n",
    "    # Initial ensemble model\n",
    "    state_size = np.prod(env.observation_space.shape)\n",
    "    action_size = np.prod(env.action_space.shape)\n",
    "    print(env.action_space)\n",
    "    if args.model_type == 'pytorch':\n",
    "        env_model = EnsembleDynamicsModel(args.num_networks, args.num_elites, state_size, action_size, args.reward_size, args.pred_hidden_size,\n",
    "                                          use_decay=args.use_decay)\n",
    "    else:\n",
    "        env_model = construct_model(obs_dim=state_size, act_dim=action_size, hidden_dim=args.pred_hidden_size, num_networks=args.num_networks,\n",
    "                                    num_elites=args.num_elites)\n",
    "\n",
    "    \n",
    "    # Predict environments\n",
    "    predict_env = PredictEnv(env_model, args.model_type)\n",
    "\n",
    "   \n",
    "    # Initial pool for env\n",
    "    env_pool = ReplayMemory(args.replay_size)\n",
    "    print(env_pool)\n",
    "    \n",
    "    # Initial pool for model\n",
    "    rollouts_per_epoch = args.rollout_batch_size * args.epoch_length / args.model_train_freq\n",
    "    model_steps_per_epoch = int(1 * rollouts_per_epoch)\n",
    "    new_pool_size = args.model_retain_epochs * model_steps_per_epoch\n",
    "    model_pool = ReplayMemory(new_pool_size)\n",
    "    \n",
    "    \n",
    "    # Sampler of environment\n",
    "    env_sampler = EnvSampler(env)\n",
    "    print('-------------------final training!----------------------')\n",
    "    train(args, env_sampler, predict_env, agent, env_pool, model_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--cuda'], dest='cuda', nargs=0, const=True, default=True, type=None, choices=None, help='run on CUDA (default: True)', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='MBPO')\n",
    "parser.add_argument('--seed', type=int, default=123456, metavar='N',\n",
    "                    help='random seed (default: 123456)')\n",
    "\n",
    "parser.add_argument('--use_decay', type=bool, default=True, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient(τ) (default: 0.005)')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\n",
    "                    help='Temperature parameter α determines the relative importance of the entropy\\\n",
    "                        term against the reward (default: 0.2)')\n",
    "# parser.add_argument('--policy', default=\"Gaussian\",\n",
    "#                     help='Policy Type: Gaussian | Deterministic (default: Gaussian)')\n",
    "parser.add_argument('--policy', default=\"Gaussian\",\n",
    "                    help='Policy Type: Gaussian | Deterministic (default: Gaussian)')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, metavar='N',\n",
    "                    help='Value target update per no. of updates per step (default: 1)')\n",
    "parser.add_argument('--automatic_entropy_tuning', type=bool, default=False, metavar='G',\n",
    "                    help='Automaically adjust α (default: False)')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\n",
    "                    help='hidden size (default: 256)')\n",
    "parser.add_argument('--lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='learning rate (default: 0.0003)')\n",
    "\n",
    "parser.add_argument('--num_networks', type=int, default=3, metavar='E',\n",
    "                    help='ensemble size (default: 7)')\n",
    "parser.add_argument('--num_elites', type=int, default=5, metavar='E',\n",
    "                    help='elite size (default: 5)')\n",
    "parser.add_argument('--pred_hidden_size', type=int, default=200, metavar='E',\n",
    "                    help='hidden size for predictive model')\n",
    "parser.add_argument('--reward_size', type=int, default=1, metavar='E',\n",
    "                    help='environment reward size')\n",
    "\n",
    "parser.add_argument('--replay_size', type=int, default=10000, metavar='N',\n",
    "                    help='size of replay buffer (default: 10000000)')\n",
    "\n",
    "parser.add_argument('--model_retain_epochs', type=int, default=1, metavar='A',\n",
    "                    help='retain epochs')\n",
    "parser.add_argument('--model_train_freq', type=int, default=90, metavar='A',\n",
    "                    help='frequency of training')\n",
    "parser.add_argument('--rollout_batch_size', type=int, default=1000, metavar='A',\n",
    "                    help='rollout number M')\n",
    "parser.add_argument('--epoch_length', type=int, default=100, metavar='A',\n",
    "                    help='steps per epoch')\n",
    "parser.add_argument('--rollout_min_epoch', type=int, default=20, metavar='A',\n",
    "                    help='rollout min epoch')\n",
    "parser.add_argument('--rollout_max_epoch', type=int, default=150, metavar='A',\n",
    "                    help='rollout max epoch')\n",
    "parser.add_argument('--rollout_min_length', type=int, default=1, metavar='A',\n",
    "                    help='rollout min length')\n",
    "parser.add_argument('--rollout_max_length', type=int, default=15, metavar='A',\n",
    "                    help='rollout max length')\n",
    "parser.add_argument('--num_epoch', type=int, default=50, metavar='A',\n",
    "                    help='total number of epochs')\n",
    "parser.add_argument('--min_pool_size', type=int, default=100, metavar='A',\n",
    "                    help='minimum pool size')\n",
    "parser.add_argument('--real_ratio', type=float, default=0.05, metavar='A',\n",
    "                    help='ratio of env samples / model samples')\n",
    "parser.add_argument('--train_every_n_steps', type=int, default=1, metavar='A',\n",
    "                    help='frequency of training policy')\n",
    "parser.add_argument('--num_train_repeat', type=int, default=20, metavar='A',\n",
    "                    help='times to training policy per step')\n",
    "parser.add_argument('--max_train_repeat_per_step', type=int, default=5, metavar='A',\n",
    "                    help='max training times per step')\n",
    "parser.add_argument('--policy_train_batch_size', type=int, default=256, metavar='A',\n",
    "                    help='batch size for training policy')\n",
    "parser.add_argument('--init_exploration_steps', type=int, default=1000, metavar='A',\n",
    "                    help='exploration steps initially')\n",
    "\n",
    "parser.add_argument('--model_type', default='pytorch', metavar='A',\n",
    "                    help='predict model -- pytorch or tensorflow')\n",
    "\n",
    "parser.add_argument('--cuda', default=True, action=\"store_true\",\n",
    "                    help='run on CUDA (default: True)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(alpha=0.2, automatic_entropy_tuning=False, cuda=True, epoch_length=100, gamma=0.99, hidden_size=256, init_exploration_steps=1000, lr=0.0003, max_train_repeat_per_step=5, min_pool_size=100, model_retain_epochs=1, model_train_freq=90, model_type='pytorch', num_elites=5, num_epoch=50, num_networks=3, num_train_repeat=20, policy='Gaussian', policy_train_batch_size=256, pred_hidden_size=200, real_ratio=0.05, replay_size=10000, reward_size=1, rollout_batch_size=1000, rollout_max_epoch=150, rollout_max_length=15, rollout_min_epoch=20, rollout_min_length=1, seed=123456, target_update_interval=1, tau=0.005, train_every_n_steps=1, use_decay=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "<sac.replay_memory.ReplayMemory object at 0x00000140A42AB108>\n",
      "-------------------final training!----------------------\n",
      "done exploration before starting\n",
      "epoch_step: 0\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, holdout mse losses: [46.29018 46.4403  46.31322]\n",
      "epoch: 1, holdout mse losses: [44.89128 45.46177 44.88377]\n",
      "epoch: 2, holdout mse losses: [40.81736 42.25771 40.60838]\n",
      "epoch: 3, holdout mse losses: [33.08612 35.66463 32.69725]\n",
      "epoch: 4, holdout mse losses: [24.03356 27.55015 23.49522]\n",
      "epoch: 5, holdout mse losses: [16.5349  24.08055 18.39691]\n",
      "epoch: 6, holdout mse losses: [12.99663 22.17967 17.59348]\n",
      "epoch: 7, holdout mse losses: [13.91541 18.83678 14.42812]\n",
      "epoch: 8, holdout mse losses: [ 7.20264 14.8828   8.55309]\n",
      "epoch: 9, holdout mse losses: [ 2.64997 11.49969  4.29515]\n",
      "epoch: 10, holdout mse losses: [1.56292 8.27298 1.86567]\n",
      "epoch: 11, holdout mse losses: [1.74043 4.41443 1.46596]\n",
      "epoch: 12, holdout mse losses: [1.37803 1.5441  1.56998]\n",
      "epoch: 13, holdout mse losses: [0.76081 1.21147 0.92863]\n",
      "epoch: 14, holdout mse losses: [0.38997 1.57873 0.44257]\n",
      "epoch: 15, holdout mse losses: [0.3029  0.88416 0.35642]\n",
      "epoch: 16, holdout mse losses: [0.31688 0.44904 0.35005]\n",
      "epoch: 17, holdout mse losses: [0.23379 0.43724 0.27622]\n",
      "epoch: 18, holdout mse losses: [0.18588 0.40011 0.20756]\n",
      "epoch: 19, holdout mse losses: [0.19891 0.28546 0.20318]\n",
      "epoch: 20, holdout mse losses: [0.18915 0.20434 0.20838]\n",
      "epoch: 21, holdout mse losses: [0.16419 0.18494 0.17906]\n",
      "epoch: 22, holdout mse losses: [0.1417  0.19583 0.14569]\n",
      "epoch: 23, holdout mse losses: [0.1267  0.1834  0.15179]\n",
      "epoch: 24, holdout mse losses: [0.12096 0.15171 0.14632]\n",
      "epoch: 25, holdout mse losses: [0.12495 0.13598 0.12385]\n",
      "epoch: 26, holdout mse losses: [0.15056 0.12823 0.11256]\n",
      "epoch: 27, holdout mse losses: [0.14219 0.12167 0.11393]\n",
      "epoch: 28, holdout mse losses: [0.12716 0.12151 0.10742]\n",
      "epoch: 29, holdout mse losses: [0.14759 0.13259 0.09962]\n",
      "epoch: 30, holdout mse losses: [0.11307 0.16241 0.09668]\n",
      "epoch: 31, holdout mse losses: [0.08648 0.17718 0.09218]\n",
      "epoch: 32, holdout mse losses: [0.08348 0.15284 0.0895 ]\n",
      "epoch: 33, holdout mse losses: [0.09355 0.11901 0.08291]\n",
      "epoch: 34, holdout mse losses: [0.10236 0.10182 0.08296]\n",
      "epoch: 35, holdout mse losses: [0.11349 0.10105 0.10226]\n",
      "epoch: 36, holdout mse losses: [0.08653 0.09733 0.19396]\n",
      "epoch: 37, holdout mse losses: [0.08897 0.08806 0.19435]\n",
      "epoch: 38, holdout mse losses: [0.08542 0.08998 0.12324]\n",
      "epoch: 39, holdout mse losses: [0.07157 0.09778 0.07971]\n",
      "epoch: 40, holdout mse losses: [0.0715  0.09949 0.09444]\n",
      "epoch: 41, holdout mse losses: [0.07744 0.09114 0.1115 ]\n",
      "epoch: 42, holdout mse losses: [0.11736 0.08471 0.1021 ]\n",
      "epoch: 43, holdout mse losses: [0.08564 0.08213 0.08248]\n",
      "epoch: 44, holdout mse losses: [0.07489 0.08169 0.0704 ]\n",
      "epoch: 45, holdout mse losses: [0.08746 0.08142 0.06991]\n",
      "epoch: 46, holdout mse losses: [0.0661  0.0811  0.07049]\n",
      "epoch: 47, holdout mse losses: [0.07106 0.0796  0.06552]\n",
      "epoch: 48, holdout mse losses: [0.07094 0.08268 0.07222]\n",
      "epoch: 49, holdout mse losses: [0.07767 0.0794  0.06799]\n",
      "epoch: 50, holdout mse losses: [0.27506 0.0785  0.06831]\n",
      "epoch: 51, holdout mse losses: [1.05572 0.08293 0.06369]\n",
      "epoch: 52, holdout mse losses: [1.24674 0.073   0.0613 ]\n",
      "epoch: 53, holdout mse losses: [1.16396 0.07108 0.06781]\n",
      "epoch: 54, holdout mse losses: [0.81781 0.07513 0.07699]\n",
      "epoch: 55, holdout mse losses: [0.37771 0.0721  0.06484]\n",
      "epoch: 56, holdout mse losses: [0.18217 0.06545 0.05549]\n",
      "epoch: 57, holdout mse losses: [0.13063 0.06204 0.06599]\n",
      "epoch: 58, holdout mse losses: [0.17951 0.06113 0.06935]\n",
      "epoch: 59, holdout mse losses: [0.16448 0.06098 0.06647]\n",
      "epoch: 60, holdout mse losses: [0.13658 0.06062 0.06138]\n",
      "epoch: 61, holdout mse losses: [0.08686 0.05666 0.05413]\n",
      "epoch: 62, holdout mse losses: [0.05798 0.05575 0.05317]\n",
      "epoch: 63, holdout mse losses: [0.05929 0.06295 0.05122]\n",
      "epoch: 64, holdout mse losses: [0.05333 0.05931 0.04988]\n",
      "epoch: 65, holdout mse losses: [0.0476  0.06408 0.09099]\n",
      "epoch: 66, holdout mse losses: [0.04221 0.06541 0.12548]\n",
      "epoch: 67, holdout mse losses: [0.04068 0.06166 0.10304]\n",
      "epoch: 68, holdout mse losses: [0.04343 0.06264 0.0639 ]\n",
      "epoch: 69, holdout mse losses: [0.04393 0.06924 0.06232]\n",
      "epoch: 70, holdout mse losses: [0.04494 0.06024 0.07902]\n",
      "epoch: 71, holdout mse losses: [0.04266 0.10973 0.08085]\n",
      "epoch: 72, holdout mse losses: [0.03965 0.10939 0.06566]\n",
      "epoch: 73, holdout mse losses: [0.03973 0.10058 0.05324]\n",
      "epoch: 74, holdout mse losses: [0.03941 0.07053 0.04726]\n",
      "epoch: 75, holdout mse losses: [0.03928 0.04092 0.04781]\n",
      "epoch: 76, holdout mse losses: [0.03887 0.04553 0.05058]\n",
      "epoch: 77, holdout mse losses: [0.03909 0.04239 0.05541]\n",
      "epoch: 78, holdout mse losses: [0.03959 0.04821 0.05704]\n",
      "epoch: 79, holdout mse losses: [0.04133 0.06303 0.0567 ]\n",
      "epoch: 80, holdout mse losses: [0.04112 0.03809 0.0576 ]\n",
      "epoch: 81, holdout mse losses: [0.04106 0.04613 0.2327 ]\n",
      "epoch: 82, holdout mse losses: [0.51142 0.02764 0.5269 ]\n",
      "epoch: 83, holdout mse losses: [0.21589 0.03062 0.54016]\n",
      "epoch: 84, holdout mse losses: [0.25072 0.02609 0.31668]\n",
      "epoch: 85, holdout mse losses: [0.36088 0.02963 0.12723]\n",
      "epoch: 86, holdout mse losses: [0.16445 0.02574 0.11624]\n",
      "epoch: 87, holdout mse losses: [0.08297 0.03206 0.16059]\n",
      "epoch: 88, holdout mse losses: [0.12792 0.02668 0.13301]\n",
      "epoch: 89, holdout mse losses: [0.08911 0.03586 0.06457]\n",
      "epoch: 90, holdout mse losses: [0.06514 0.05248 0.04462]\n",
      "epoch: 91, holdout mse losses: [0.08194 0.02548 0.05336]\n",
      "epoch: 92, holdout mse losses: [0.07534 0.02652 0.05422]\n",
      "epoch: 93, holdout mse losses: [0.06095 0.02632 0.04566]\n",
      "epoch: 94, holdout mse losses: [0.05889 0.02291 0.04425]\n",
      "epoch: 95, holdout mse losses: [0.05536 0.02167 0.0479 ]\n",
      "epoch: 96, holdout mse losses: [0.05232 0.02028 0.04656]\n",
      "epoch: 97, holdout mse losses: [0.05164 0.01935 0.04471]\n",
      "epoch: 98, holdout mse losses: [0.04866 0.02237 0.04184]\n",
      "epoch: 99, holdout mse losses: [0.04634 0.02759 0.03917]\n",
      "epoch: 100, holdout mse losses: [0.0445  0.02143 0.03749]\n",
      "epoch: 101, holdout mse losses: [0.0484  0.01804 0.03853]\n",
      "epoch: 102, holdout mse losses: [0.04117 0.02032 0.04011]\n",
      "epoch: 103, holdout mse losses: [0.04235 0.0531  0.03908]\n",
      "epoch: 104, holdout mse losses: [0.03453 0.08241 0.03903]\n",
      "epoch: 105, holdout mse losses: [0.0327  0.19626 0.03771]\n",
      "epoch: 106, holdout mse losses: [0.03222 0.07979 0.03539]\n",
      "epoch: 107, holdout mse losses: [0.03285 0.07985 0.03487]\n",
      "epoch: 108, holdout mse losses: [0.03133 0.11524 0.03305]\n",
      "epoch: 109, holdout mse losses: [0.0292  0.06428 0.03349]\n",
      "epoch: 110, holdout mse losses: [0.02688 0.02279 0.03145]\n",
      "epoch: 111, holdout mse losses: [0.03414 0.03425 0.02954]\n",
      "epoch: 112, holdout mse losses: [0.02152 0.01927 0.02687]\n",
      "epoch: 113, holdout mse losses: [0.03223 0.02571 0.02742]\n",
      "epoch: 114, holdout mse losses: [0.03094 0.02778 0.02768]\n",
      "epoch: 115, holdout mse losses: [0.02173 0.02824 0.02757]\n",
      "epoch: 116, holdout mse losses: [0.02329 0.02022 0.0535 ]\n",
      "epoch: 117, holdout mse losses: [0.02588 0.01698 0.03387]\n",
      "epoch: 118, holdout mse losses: [0.02183 0.01733 0.04098]\n",
      "epoch: 119, holdout mse losses: [0.02502 0.01878 0.02603]\n",
      "epoch: 120, holdout mse losses: [0.02242 0.01741 0.03304]\n",
      "epoch: 121, holdout mse losses: [0.02146 0.01616 0.02495]\n",
      "epoch: 122, holdout mse losses: [0.02946 0.01653 0.02153]\n",
      "epoch: 123, holdout mse losses: [0.07589 0.0248  0.0197 ]\n",
      "epoch: 124, holdout mse losses: [0.06045 0.01917 0.02223]\n",
      "epoch: 125, holdout mse losses: [0.03507 0.02447 0.02017]\n",
      "epoch: 126, holdout mse losses: [0.03135 0.01668 0.02503]\n",
      "epoch: 127, holdout mse losses: [0.03924 0.02078 0.02695]\n",
      "epoch: 128, holdout mse losses: [0.02174 0.01709 0.01583]\n",
      "epoch: 129, holdout mse losses: [0.01503 0.02013 0.03804]\n",
      "epoch: 130, holdout mse losses: [0.01639 0.01779 0.04453]\n",
      "epoch: 131, holdout mse losses: [0.02105 0.01588 0.02571]\n",
      "epoch: 132, holdout mse losses: [0.02019 0.01589 0.05901]\n",
      "epoch: 133, holdout mse losses: [0.01678 0.01524 0.03121]\n",
      "epoch: 134, holdout mse losses: [0.01403 0.03598 0.04177]\n",
      "epoch: 135, holdout mse losses: [0.01233 0.01963 0.07101]\n",
      "epoch: 136, holdout mse losses: [0.0132  0.02317 0.02665]\n",
      "epoch: 137, holdout mse losses: [0.01393 0.04268 0.02511]\n",
      "epoch: 138, holdout mse losses: [0.01142 0.01748 0.02142]\n",
      "epoch: 139, holdout mse losses: [0.01301 0.02837 0.02162]\n",
      "epoch: 140, holdout mse losses: [0.01547 0.02815 0.01792]\n",
      "epoch: 141, holdout mse losses: [0.01498 0.02711 0.02426]\n",
      "epoch: 142, holdout mse losses: [0.01283 0.01721 0.01969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 143, holdout mse losses: [0.01414 0.01883 0.01518]\n",
      "epoch: 144, holdout mse losses: [0.01461 0.0156  0.01404]\n",
      "epoch: 145, holdout mse losses: [0.01689 0.01402 0.01503]\n",
      "epoch: 146, holdout mse losses: [0.01473 0.01505 0.01493]\n",
      "epoch: 147, holdout mse losses: [0.01464 0.01213 0.03312]\n",
      "epoch: 148, holdout mse losses: [0.0179  0.01467 0.01662]\n",
      "epoch: 149, holdout mse losses: [0.02837 0.01282 0.01974]\n",
      "epoch: 150, holdout mse losses: [0.0132  0.01166 0.02585]\n",
      "epoch: 151, holdout mse losses: [0.01438 0.01082 0.02037]\n",
      "epoch: 152, holdout mse losses: [0.0157  0.01105 0.02425]\n",
      "epoch: 153, holdout mse losses: [0.01133 0.01175 0.02268]\n",
      "epoch: 154, holdout mse losses: [0.01664 0.01109 0.01747]\n",
      "epoch: 155, holdout mse losses: [0.01183 0.01163 0.6836 ]\n",
      "epoch: 156, holdout mse losses: [0.01027 0.01113 0.25629]\n",
      "epoch: 157, holdout mse losses: [0.01003 0.01123 0.60019]\n",
      "epoch: 158, holdout mse losses: [0.00983 0.01297 0.10296]\n",
      "epoch: 159, holdout mse losses: [0.0107  0.02174 0.13195]\n",
      "epoch: 160, holdout mse losses: [0.01079 0.02148 0.17477]\n",
      "epoch: 161, holdout mse losses: [0.01238 0.0219  0.05765]\n",
      "epoch: 162, holdout mse losses: [0.03984 0.01926 0.129  ]\n",
      "epoch: 163, holdout mse losses: [0.03016 0.01731 0.0682 ]\n",
      "total step:100\n",
      "100 90.07339090108871\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 1\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.07246 0.0214  0.06557]\n",
      "epoch: 1, holdout mse losses: [0.07951 0.11162 0.0907 ]\n",
      "epoch: 2, holdout mse losses: [0.10315 0.01955 0.06826]\n",
      "epoch: 3, holdout mse losses: [0.02106 0.06897 0.04951]\n",
      "epoch: 4, holdout mse losses: [0.03346 0.0171  0.04839]\n",
      "epoch: 5, holdout mse losses: [0.02477 0.03168 0.03867]\n",
      "epoch: 6, holdout mse losses: [0.01167 0.01284 0.03026]\n",
      "epoch: 7, holdout mse losses: [0.01225 0.01818 0.03145]\n",
      "epoch: 8, holdout mse losses: [0.01128 0.01001 0.02853]\n",
      "epoch: 9, holdout mse losses: [0.01208 0.01528 0.03026]\n",
      "epoch: 10, holdout mse losses: [0.01079 0.01002 0.03199]\n",
      "epoch: 11, holdout mse losses: [0.01063 0.01048 0.03251]\n",
      "epoch: 12, holdout mse losses: [0.00996 0.00877 0.03   ]\n",
      "epoch: 13, holdout mse losses: [0.00919 0.00901 0.03729]\n",
      "epoch: 14, holdout mse losses: [0.00891 0.04617 0.04013]\n",
      "epoch: 15, holdout mse losses: [0.0095  0.04642 0.02434]\n",
      "epoch: 16, holdout mse losses: [0.00884 0.01855 0.02034]\n",
      "epoch: 17, holdout mse losses: [0.00863 0.05439 0.02357]\n",
      "epoch: 18, holdout mse losses: [0.00854 0.03245 0.021  ]\n",
      "epoch: 19, holdout mse losses: [0.00832 0.04526 0.01739]\n",
      "epoch: 20, holdout mse losses: [0.00864 0.01862 0.01659]\n",
      "epoch: 21, holdout mse losses: [0.00879 0.02943 0.01429]\n",
      "epoch: 22, holdout mse losses: [0.00889 0.0145  0.01507]\n",
      "epoch: 23, holdout mse losses: [0.00993 0.01492 0.01408]\n",
      "epoch: 24, holdout mse losses: [0.01165 0.01309 0.01326]\n",
      "epoch: 25, holdout mse losses: [0.01328 0.01267 0.01358]\n",
      "epoch: 26, holdout mse losses: [0.01887 0.01039 0.01488]\n",
      "epoch: 27, holdout mse losses: [0.01391 0.01687 0.01633]\n",
      "epoch: 28, holdout mse losses: [0.01404 0.00969 0.01826]\n",
      "epoch: 29, holdout mse losses: [0.01499 0.01287 0.0186 ]\n",
      "total step:200\n",
      "200 86.35754477977753\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 2\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01812 0.02065 0.0506 ]\n",
      "epoch: 1, holdout mse losses: [0.07426 0.01216 0.15683]\n",
      "epoch: 2, holdout mse losses: [0.20153 0.04489 0.04298]\n",
      "epoch: 3, holdout mse losses: [0.19961 0.02    0.06203]\n",
      "epoch: 4, holdout mse losses: [0.16309 0.01331 0.05964]\n",
      "epoch: 5, holdout mse losses: [0.16548 0.01405 0.02325]\n",
      "epoch: 6, holdout mse losses: [0.11784 0.01067 0.04297]\n",
      "epoch: 7, holdout mse losses: [0.02306 0.01938 0.02089]\n",
      "epoch: 8, holdout mse losses: [0.03064 0.01466 0.01765]\n",
      "epoch: 9, holdout mse losses: [0.02586 0.012   0.01792]\n",
      "epoch: 10, holdout mse losses: [0.01897 0.0084  0.01539]\n",
      "epoch: 11, holdout mse losses: [0.0151  0.00652 0.01037]\n",
      "epoch: 12, holdout mse losses: [0.01492 0.00603 0.01431]\n",
      "epoch: 13, holdout mse losses: [0.01369 0.00745 0.01365]\n",
      "epoch: 14, holdout mse losses: [0.01163 0.00882 0.0131 ]\n",
      "epoch: 15, holdout mse losses: [0.011   0.00892 0.02283]\n",
      "epoch: 16, holdout mse losses: [0.0098  0.01332 0.01513]\n",
      "epoch: 17, holdout mse losses: [0.00937 0.01951 0.01294]\n",
      "epoch: 18, holdout mse losses: [0.01492 0.01932 0.01138]\n",
      "epoch: 19, holdout mse losses: [0.01046 0.02471 0.025  ]\n",
      "epoch: 20, holdout mse losses: [0.01591 0.14159 0.01495]\n",
      "epoch: 21, holdout mse losses: [0.01753 0.1796  0.02326]\n",
      "epoch: 22, holdout mse losses: [0.01882 0.10675 0.02717]\n",
      "total step:300\n",
      "300 91.65258014202118\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 3\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01    0.06657 0.01774]\n",
      "epoch: 1, holdout mse losses: [0.01315 0.04659 0.01426]\n",
      "epoch: 2, holdout mse losses: [0.00943 0.0307  0.00842]\n",
      "epoch: 3, holdout mse losses: [0.01041 0.02646 0.00872]\n",
      "epoch: 4, holdout mse losses: [0.00791 0.0171  0.01331]\n",
      "epoch: 5, holdout mse losses: [0.00769 0.01712 0.01001]\n",
      "epoch: 6, holdout mse losses: [0.00829 0.01723 0.01104]\n",
      "epoch: 7, holdout mse losses: [0.0078  0.01853 0.01003]\n",
      "epoch: 8, holdout mse losses: [0.00699 0.01507 0.00978]\n",
      "epoch: 9, holdout mse losses: [0.00674 0.01218 0.0138 ]\n",
      "epoch: 10, holdout mse losses: [0.00717 0.01101 0.00978]\n",
      "epoch: 11, holdout mse losses: [0.00806 0.00948 0.01032]\n",
      "epoch: 12, holdout mse losses: [0.01054 0.0089  0.01954]\n",
      "epoch: 13, holdout mse losses: [0.00878 0.00913 0.02082]\n",
      "epoch: 14, holdout mse losses: [0.01003 0.0115  0.02577]\n",
      "epoch: 15, holdout mse losses: [0.00812 0.02277 0.01099]\n",
      "epoch: 16, holdout mse losses: [0.00849 0.0115  0.01945]\n",
      "epoch: 17, holdout mse losses: [0.00802 0.02729 0.01135]\n",
      "total step:400\n",
      "400 90.85182392597198\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 4\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01183 0.01705 0.01408]\n",
      "epoch: 1, holdout mse losses: [0.0173  0.01524 0.02179]\n",
      "epoch: 2, holdout mse losses: [0.0107  0.01182 0.0099 ]\n",
      "epoch: 3, holdout mse losses: [0.0196  0.00968 0.00873]\n",
      "epoch: 4, holdout mse losses: [0.01046 0.00888 0.00777]\n",
      "epoch: 5, holdout mse losses: [0.01066 0.00788 0.01065]\n",
      "epoch: 6, holdout mse losses: [0.0065  0.00758 0.00785]\n",
      "epoch: 7, holdout mse losses: [0.00703 0.00673 0.00866]\n",
      "epoch: 8, holdout mse losses: [0.00754 0.00653 0.00876]\n",
      "epoch: 9, holdout mse losses: [0.00598 0.0063  0.00861]\n",
      "epoch: 10, holdout mse losses: [0.00644 0.00607 0.01206]\n",
      "epoch: 11, holdout mse losses: [0.00574 0.00603 0.01002]\n",
      "epoch: 12, holdout mse losses: [0.00626 0.00657 0.01096]\n",
      "epoch: 13, holdout mse losses: [0.0205  0.00643 0.00877]\n",
      "epoch: 14, holdout mse losses: [0.0152  0.00665 0.00725]\n",
      "epoch: 15, holdout mse losses: [0.11474 0.00638 0.0065 ]\n",
      "epoch: 16, holdout mse losses: [0.07006 0.00651 0.00771]\n",
      "epoch: 17, holdout mse losses: [0.09072 0.01741 0.00742]\n",
      "epoch: 18, holdout mse losses: [0.0456  0.01678 0.00823]\n",
      "epoch: 19, holdout mse losses: [0.02627 0.02862 0.0076 ]\n",
      "epoch: 20, holdout mse losses: [0.03774 0.01509 0.01592]\n",
      "total step:500\n",
      "500 87.55347275733948\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 5\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01424 0.02699 0.06199]\n",
      "epoch: 1, holdout mse losses: [0.0427  0.07164 0.07573]\n",
      "epoch: 2, holdout mse losses: [0.01755 0.04262 0.03514]\n",
      "epoch: 3, holdout mse losses: [0.01578 0.01074 0.02099]\n",
      "epoch: 4, holdout mse losses: [0.01528 0.01484 0.01535]\n",
      "epoch: 5, holdout mse losses: [0.01085 0.00723 0.02063]\n",
      "epoch: 6, holdout mse losses: [0.01221 0.00827 0.01176]\n",
      "epoch: 7, holdout mse losses: [0.01181 0.36299 0.01108]\n",
      "epoch: 8, holdout mse losses: [0.00962 9.55946 0.00979]\n",
      "epoch: 9, holdout mse losses: [0.00911 8.37618 0.01202]\n",
      "epoch: 10, holdout mse losses: [0.0089  3.93646 0.01078]\n",
      "epoch: 11, holdout mse losses: [0.00935 0.53743 0.01004]\n",
      "epoch: 12, holdout mse losses: [0.01736 0.81986 0.00929]\n",
      "epoch: 13, holdout mse losses: [0.0246  0.95023 0.0092 ]\n",
      "epoch: 14, holdout mse losses: [0.01866 0.21385 0.00876]\n",
      "epoch: 15, holdout mse losses: [0.00869 0.25869 0.00833]\n",
      "epoch: 16, holdout mse losses: [0.01069 0.32159 0.00784]\n",
      "epoch: 17, holdout mse losses: [0.0085  0.19876 0.0076 ]\n",
      "epoch: 18, holdout mse losses: [0.00929 0.12478 0.00945]\n",
      "epoch: 19, holdout mse losses: [0.00863 0.14783 0.05188]\n",
      "epoch: 20, holdout mse losses: [0.00914 0.13805 0.07728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, holdout mse losses: [0.00822 0.11394 0.05704]\n",
      "epoch: 22, holdout mse losses: [0.00766 0.10808 0.03291]\n",
      "epoch: 23, holdout mse losses: [0.0073  0.10592 0.05442]\n",
      "epoch: 24, holdout mse losses: [0.0072  0.09496 0.04263]\n",
      "epoch: 25, holdout mse losses: [0.00713 0.08856 0.0354 ]\n",
      "epoch: 26, holdout mse losses: [0.00707 0.08566 0.02117]\n",
      "epoch: 27, holdout mse losses: [0.007   0.08173 0.01788]\n",
      "epoch: 28, holdout mse losses: [0.00724 0.07827 0.02092]\n",
      "epoch: 29, holdout mse losses: [0.00715 0.07736 0.01489]\n",
      "epoch: 30, holdout mse losses: [0.00724 0.075   0.01576]\n",
      "epoch: 31, holdout mse losses: [0.00739 0.07151 0.01431]\n",
      "total step:600\n",
      "600 89.53636461496353\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 6\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.11722 0.06916 0.01055]\n",
      "epoch: 1, holdout mse losses: [0.09446 0.06652 0.0117 ]\n",
      "epoch: 2, holdout mse losses: [0.02886 0.05933 0.01249]\n",
      "epoch: 3, holdout mse losses: [0.01616 0.05544 0.00983]\n",
      "epoch: 4, holdout mse losses: [0.01225 0.05039 0.00922]\n",
      "epoch: 5, holdout mse losses: [0.01052 0.05071 0.00889]\n",
      "epoch: 6, holdout mse losses: [0.01138 0.04758 0.00819]\n",
      "epoch: 7, holdout mse losses: [0.00835 0.04456 0.00814]\n",
      "epoch: 8, holdout mse losses: [0.0069  0.04194 0.00803]\n",
      "epoch: 9, holdout mse losses: [0.00782 0.04111 0.00779]\n",
      "epoch: 10, holdout mse losses: [0.00765 0.03985 0.00789]\n",
      "epoch: 11, holdout mse losses: [0.00725 0.04477 0.00755]\n",
      "epoch: 12, holdout mse losses: [0.00656 0.04739 0.00709]\n",
      "epoch: 13, holdout mse losses: [0.00638 0.05911 0.00666]\n",
      "epoch: 14, holdout mse losses: [0.00709 0.05007 0.01126]\n",
      "epoch: 15, holdout mse losses: [0.00782 0.03959 0.0237 ]\n",
      "epoch: 16, holdout mse losses: [0.00969 0.03884 0.01838]\n",
      "epoch: 17, holdout mse losses: [0.03777 0.03139 0.04707]\n",
      "epoch: 18, holdout mse losses: [0.07041 0.02808 0.00906]\n",
      "epoch: 19, holdout mse losses: [0.04298 0.02699 0.00749]\n",
      "epoch: 20, holdout mse losses: [0.05547 0.02701 0.00707]\n",
      "epoch: 21, holdout mse losses: [0.03537 0.0258  0.00526]\n",
      "epoch: 22, holdout mse losses: [0.01013 0.02656 0.00793]\n",
      "epoch: 23, holdout mse losses: [0.01964 0.02488 0.00711]\n",
      "epoch: 24, holdout mse losses: [0.00969 0.04152 0.00451]\n",
      "epoch: 25, holdout mse losses: [0.00787 0.07759 0.00525]\n",
      "epoch: 26, holdout mse losses: [0.00905 0.04453 0.00519]\n",
      "epoch: 27, holdout mse losses: [0.00915 0.04633 0.13429]\n",
      "epoch: 28, holdout mse losses: [0.0086  0.03947 0.57558]\n",
      "epoch: 29, holdout mse losses: [0.00824 0.03137 0.32769]\n",
      "total step:700\n",
      "700 83.99801701307297\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 7\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01865 0.02376 0.13973]\n",
      "epoch: 1, holdout mse losses: [0.00808 0.03299 0.06815]\n",
      "epoch: 2, holdout mse losses: [0.00832 0.02589 0.02823]\n",
      "epoch: 3, holdout mse losses: [0.18499 0.02148 0.03091]\n",
      "epoch: 4, holdout mse losses: [0.02878 0.02236 0.02796]\n",
      "epoch: 5, holdout mse losses: [0.10876 0.01962 0.03121]\n",
      "epoch: 6, holdout mse losses: [0.02911 0.01888 0.0273 ]\n",
      "epoch: 7, holdout mse losses: [0.01738 0.01868 0.02495]\n",
      "epoch: 8, holdout mse losses: [0.01391 0.01826 0.02021]\n",
      "epoch: 9, holdout mse losses: [0.01176 0.01795 0.02002]\n",
      "epoch: 10, holdout mse losses: [0.01087 0.01746 0.01944]\n",
      "epoch: 11, holdout mse losses: [0.01288 0.01715 0.01912]\n",
      "epoch: 12, holdout mse losses: [0.00831 0.01698 0.01991]\n",
      "epoch: 13, holdout mse losses: [0.00898 0.01741 0.02027]\n",
      "epoch: 14, holdout mse losses: [0.01188 0.0169  0.02083]\n",
      "epoch: 15, holdout mse losses: [0.01095 0.01653 0.06404]\n",
      "epoch: 16, holdout mse losses: [0.00876 0.01607 0.02516]\n",
      "epoch: 17, holdout mse losses: [0.00765 0.01625 0.04115]\n",
      "epoch: 18, holdout mse losses: [0.00719 0.01606 0.03231]\n",
      "epoch: 19, holdout mse losses: [0.00776 0.01555 0.03359]\n",
      "epoch: 20, holdout mse losses: [0.00786 0.01536 0.03624]\n",
      "epoch: 21, holdout mse losses: [0.01267 0.01551 0.02164]\n",
      "epoch: 22, holdout mse losses: [0.00732 0.01666 0.02124]\n",
      "epoch: 23, holdout mse losses: [0.00671 0.01605 0.01965]\n",
      "epoch: 24, holdout mse losses: [0.00649 0.0156  0.01808]\n",
      "epoch: 25, holdout mse losses: [0.00616 0.01537 0.01598]\n",
      "epoch: 26, holdout mse losses: [0.00662 0.01532 0.01557]\n",
      "epoch: 27, holdout mse losses: [0.0061  0.01508 0.01525]\n",
      "epoch: 28, holdout mse losses: [0.00677 0.01488 0.01453]\n",
      "epoch: 29, holdout mse losses: [0.00557 0.01529 0.01472]\n",
      "epoch: 30, holdout mse losses: [0.00767 0.01543 0.02015]\n",
      "epoch: 31, holdout mse losses: [0.0096  0.01554 0.04769]\n",
      "epoch: 32, holdout mse losses: [0.00566 0.01701 0.03762]\n",
      "epoch: 33, holdout mse losses: [0.00764 0.01609 0.0195 ]\n",
      "epoch: 34, holdout mse losses: [0.00656 0.01853 0.01937]\n",
      "total step:800\n",
      "800 96.50636780261993\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 8\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01306 0.24976 0.01073]\n",
      "epoch: 1, holdout mse losses: [0.01015 0.26367 0.02674]\n",
      "epoch: 2, holdout mse losses: [0.01037 0.27922 0.00887]\n",
      "epoch: 3, holdout mse losses: [0.00885 0.13065 0.00921]\n",
      "epoch: 4, holdout mse losses: [0.01058 0.0875  0.00672]\n",
      "epoch: 5, holdout mse losses: [0.0085  0.05924 0.00666]\n",
      "epoch: 6, holdout mse losses: [0.00801 0.04425 0.00602]\n",
      "epoch: 7, holdout mse losses: [0.00823 0.04243 0.00607]\n",
      "epoch: 8, holdout mse losses: [0.00867 0.03521 0.00638]\n",
      "epoch: 9, holdout mse losses: [0.00784 0.03256 0.00649]\n",
      "epoch: 10, holdout mse losses: [0.00744 0.03106 0.00659]\n",
      "epoch: 11, holdout mse losses: [0.00757 0.03194 0.00655]\n",
      "epoch: 12, holdout mse losses: [0.02551 0.03196 0.00661]\n",
      "epoch: 13, holdout mse losses: [0.01482 0.02844 0.00673]\n",
      "epoch: 14, holdout mse losses: [0.01517 0.02522 0.008  ]\n",
      "epoch: 15, holdout mse losses: [0.01451 0.02458 0.00732]\n",
      "epoch: 16, holdout mse losses: [0.00755 0.02462 0.0083 ]\n",
      "epoch: 17, holdout mse losses: [0.01144 0.02435 0.00837]\n",
      "epoch: 18, holdout mse losses: [0.00586 0.02346 0.01035]\n",
      "epoch: 19, holdout mse losses: [0.0081  0.02215 0.00998]\n",
      "epoch: 20, holdout mse losses: [0.00811 0.02578 0.01147]\n",
      "epoch: 21, holdout mse losses: [0.0084  0.05836 0.01717]\n",
      "epoch: 22, holdout mse losses: [0.00693 0.04536 0.00812]\n",
      "epoch: 23, holdout mse losses: [0.00632 0.02642 0.00671]\n",
      "epoch: 24, holdout mse losses: [0.00707 0.02267 0.008  ]\n",
      "epoch: 25, holdout mse losses: [0.00529 0.02299 0.00934]\n",
      "epoch: 26, holdout mse losses: [0.00573 0.0223  0.01356]\n",
      "epoch: 27, holdout mse losses: [0.00509 0.0204  0.00806]\n",
      "epoch: 28, holdout mse losses: [0.00555 0.01909 0.00925]\n",
      "epoch: 29, holdout mse losses: [0.00634 0.01867 0.01131]\n",
      "epoch: 30, holdout mse losses: [0.00636 0.01868 0.00741]\n",
      "epoch: 31, holdout mse losses: [0.00442 0.01753 0.01035]\n",
      "epoch: 32, holdout mse losses: [0.00514 0.01658 0.01014]\n",
      "epoch: 33, holdout mse losses: [0.00516 0.01558 0.00885]\n",
      "epoch: 34, holdout mse losses: [0.01326 0.01784 0.00646]\n",
      "epoch: 35, holdout mse losses: [0.10267 0.03976 0.00623]\n",
      "epoch: 36, holdout mse losses: [0.01195 0.05808 0.00936]\n",
      "epoch: 37, holdout mse losses: [0.01665 0.02398 0.00861]\n",
      "epoch: 38, holdout mse losses: [0.00918 0.01792 0.00432]\n",
      "epoch: 39, holdout mse losses: [0.03681 0.02022 0.00441]\n",
      "epoch: 40, holdout mse losses: [0.00964 0.02174 0.00599]\n",
      "epoch: 41, holdout mse losses: [0.0037  0.01482 0.00719]\n",
      "epoch: 42, holdout mse losses: [0.00632 0.01371 0.00414]\n",
      "epoch: 43, holdout mse losses: [0.00627 0.01281 0.02637]\n",
      "epoch: 44, holdout mse losses: [0.00599 0.01383 0.11485]\n",
      "epoch: 45, holdout mse losses: [0.00522 0.01315 0.12095]\n",
      "epoch: 46, holdout mse losses: [0.00541 0.01196 0.05298]\n",
      "epoch: 47, holdout mse losses: [0.00999 0.01185 0.02664]\n",
      "epoch: 48, holdout mse losses: [0.00639 0.01185 0.02625]\n",
      "epoch: 49, holdout mse losses: [0.0138  0.01116 0.01084]\n",
      "epoch: 50, holdout mse losses: [0.01448 0.0106  0.0059 ]\n",
      "epoch: 51, holdout mse losses: [0.00589 0.01073 0.0046 ]\n",
      "epoch: 52, holdout mse losses: [0.0041  0.01034 0.00428]\n",
      "epoch: 53, holdout mse losses: [0.00347 0.01002 0.00345]\n",
      "epoch: 54, holdout mse losses: [0.00374 0.01078 0.00345]\n",
      "epoch: 55, holdout mse losses: [0.00327 0.01087 0.00316]\n",
      "epoch: 56, holdout mse losses: [0.0038  0.01109 0.00323]\n",
      "epoch: 57, holdout mse losses: [0.00315 0.00929 0.00313]\n",
      "epoch: 58, holdout mse losses: [0.00339 0.00991 0.00393]\n",
      "epoch: 59, holdout mse losses: [0.04211 0.00885 0.01546]\n",
      "epoch: 60, holdout mse losses: [0.16488 0.00822 0.2933 ]\n",
      "epoch: 61, holdout mse losses: [0.02021 0.00816 0.02836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 62, holdout mse losses: [0.07036 0.00756 0.05929]\n",
      "epoch: 63, holdout mse losses: [0.02204 0.00746 0.00447]\n",
      "epoch: 64, holdout mse losses: [0.01639 0.00958 0.01954]\n",
      "epoch: 65, holdout mse losses: [0.01478 0.00842 0.00782]\n",
      "epoch: 66, holdout mse losses: [0.00548 0.00798 0.00699]\n",
      "epoch: 67, holdout mse losses: [0.00747 0.0077  0.00496]\n",
      "epoch: 68, holdout mse losses: [0.00639 0.00671 0.00655]\n",
      "epoch: 69, holdout mse losses: [0.00532 0.00668 0.00605]\n",
      "epoch: 70, holdout mse losses: [0.00548 0.00761 0.00401]\n",
      "epoch: 71, holdout mse losses: [0.00593 0.007   0.00433]\n",
      "epoch: 72, holdout mse losses: [0.00648 0.00717 0.00511]\n",
      "epoch: 73, holdout mse losses: [0.00427 0.00713 0.00424]\n",
      "epoch: 74, holdout mse losses: [0.00392 0.0065  0.0049 ]\n",
      "epoch: 75, holdout mse losses: [0.00451 0.00819 0.00517]\n",
      "epoch: 76, holdout mse losses: [0.005   0.00635 0.00465]\n",
      "epoch: 77, holdout mse losses: [0.00434 0.00638 0.00572]\n",
      "epoch: 78, holdout mse losses: [0.0046  0.0078  0.00541]\n",
      "epoch: 79, holdout mse losses: [0.00473 0.00671 0.00368]\n",
      "epoch: 80, holdout mse losses: [0.0048  0.00668 0.0034 ]\n",
      "epoch: 81, holdout mse losses: [0.00539 0.00616 0.00354]\n",
      "epoch: 82, holdout mse losses: [0.01058 0.00568 0.0039 ]\n",
      "epoch: 83, holdout mse losses: [0.06977 0.00756 0.00369]\n",
      "epoch: 84, holdout mse losses: [0.19602 0.00831 0.00447]\n",
      "epoch: 85, holdout mse losses: [0.02865 0.00687 0.00354]\n",
      "epoch: 86, holdout mse losses: [0.13394 0.00538 0.00391]\n",
      "epoch: 87, holdout mse losses: [0.02882 0.00904 0.00365]\n",
      "epoch: 88, holdout mse losses: [0.0255  0.00638 0.00421]\n",
      "epoch: 89, holdout mse losses: [0.01763 0.01383 0.00343]\n",
      "epoch: 90, holdout mse losses: [0.00794 0.00607 0.00365]\n",
      "epoch: 91, holdout mse losses: [0.01411 0.00668 0.00318]\n",
      "epoch: 92, holdout mse losses: [0.0101  0.00918 0.00281]\n",
      "epoch: 93, holdout mse losses: [0.00911 0.00954 0.00481]\n",
      "epoch: 94, holdout mse losses: [0.00696 0.00684 0.0034 ]\n",
      "epoch: 95, holdout mse losses: [0.00805 0.00576 0.00285]\n",
      "epoch: 96, holdout mse losses: [0.00726 0.01032 0.00341]\n",
      "epoch: 97, holdout mse losses: [0.00664 0.00706 0.00275]\n",
      "epoch: 98, holdout mse losses: [0.00631 0.00693 0.00242]\n",
      "epoch: 99, holdout mse losses: [0.00668 0.00659 0.00281]\n",
      "epoch: 100, holdout mse losses: [0.00583 0.00645 0.00248]\n",
      "epoch: 101, holdout mse losses: [0.00854 0.00609 0.00264]\n",
      "epoch: 102, holdout mse losses: [0.00687 0.0066  0.00351]\n",
      "epoch: 103, holdout mse losses: [0.0057  0.0131  0.00841]\n",
      "total step:900\n",
      "900 90.47413235902786\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 9\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.00356 0.008   0.03407]\n",
      "epoch: 1, holdout mse losses: [0.00443 0.0186  0.00613]\n",
      "epoch: 2, holdout mse losses: [0.00294 0.02064 0.00335]\n",
      "epoch: 3, holdout mse losses: [0.00386 0.00862 0.00267]\n",
      "epoch: 4, holdout mse losses: [0.00314 0.00819 0.00473]\n",
      "epoch: 5, holdout mse losses: [0.00249 0.00591 0.0029 ]\n",
      "epoch: 6, holdout mse losses: [0.04865 0.00644 0.00241]\n",
      "epoch: 7, holdout mse losses: [0.23387 0.00602 0.00242]\n",
      "epoch: 8, holdout mse losses: [0.3838  0.00538 0.00219]\n",
      "epoch: 9, holdout mse losses: [0.03786 0.00511 0.00234]\n",
      "epoch: 10, holdout mse losses: [0.0411  0.00496 0.00237]\n",
      "epoch: 11, holdout mse losses: [0.02232 0.00525 0.00293]\n",
      "epoch: 12, holdout mse losses: [0.01168 0.00502 0.00262]\n",
      "epoch: 13, holdout mse losses: [0.01873 0.00826 0.02052]\n",
      "epoch: 14, holdout mse losses: [0.01522 0.01311 0.02465]\n",
      "epoch: 15, holdout mse losses: [0.00985 0.00796 0.0118 ]\n",
      "total step:1000\n",
      "1000 94.12361830472946\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 10\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01471 0.01784 0.08636]\n",
      "epoch: 1, holdout mse losses: [0.00938 0.01112 0.03536]\n",
      "epoch: 2, holdout mse losses: [0.01111 0.00785 0.01392]\n",
      "epoch: 3, holdout mse losses: [0.0077  0.00737 0.0053 ]\n",
      "epoch: 4, holdout mse losses: [0.00634 0.00658 0.00296]\n",
      "epoch: 5, holdout mse losses: [0.00542 0.00671 0.00237]\n",
      "epoch: 6, holdout mse losses: [0.00572 0.0054  0.00217]\n",
      "epoch: 7, holdout mse losses: [0.00597 0.00521 0.00228]\n",
      "epoch: 8, holdout mse losses: [0.0054  0.00555 0.00209]\n",
      "epoch: 9, holdout mse losses: [0.00657 0.00526 0.00216]\n",
      "epoch: 10, holdout mse losses: [0.00593 0.00536 0.00199]\n",
      "epoch: 11, holdout mse losses: [0.00968 0.00495 0.00203]\n",
      "epoch: 12, holdout mse losses: [0.0131  0.00515 0.00203]\n",
      "epoch: 13, holdout mse losses: [0.01163 0.00547 0.00214]\n",
      "epoch: 14, holdout mse losses: [0.00621 0.00819 0.00305]\n",
      "epoch: 15, holdout mse losses: [0.00912 0.00611 0.03261]\n",
      "epoch: 16, holdout mse losses: [0.00868 0.01838 0.00864]\n",
      "total step:1100\n",
      "1100 92.62025779485703\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 11\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.00441 0.00669 0.00582]\n",
      "epoch: 1, holdout mse losses: [0.00537 0.00544 0.00289]\n",
      "epoch: 2, holdout mse losses: [0.00432 0.00622 0.00297]\n",
      "epoch: 3, holdout mse losses: [0.00474 0.00373 0.00386]\n",
      "epoch: 4, holdout mse losses: [0.00388 0.00368 0.00195]\n",
      "epoch: 5, holdout mse losses: [0.00317 0.0034  0.00184]\n",
      "epoch: 6, holdout mse losses: [0.00312 0.00359 0.00178]\n",
      "epoch: 7, holdout mse losses: [0.00382 0.00441 0.00161]\n",
      "epoch: 8, holdout mse losses: [0.00362 0.00325 0.00186]\n",
      "epoch: 9, holdout mse losses: [0.00343 0.0035  0.00335]\n",
      "epoch: 10, holdout mse losses: [0.00361 0.00327 0.01795]\n",
      "epoch: 11, holdout mse losses: [0.00356 0.00602 0.06863]\n",
      "epoch: 12, holdout mse losses: [0.00363 0.00536 0.01542]\n",
      "epoch: 13, holdout mse losses: [0.00382 0.00725 0.00325]\n",
      "total step:1200\n",
      "1200 86.23271155357361\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 12\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01103 0.01788 0.00713]\n",
      "epoch: 1, holdout mse losses: [0.01393 0.01025 0.00583]\n",
      "epoch: 2, holdout mse losses: [0.00521 0.01258 0.00359]\n",
      "epoch: 3, holdout mse losses: [0.00727 0.00516 0.00308]\n",
      "epoch: 4, holdout mse losses: [0.00472 0.00625 0.0032 ]\n",
      "epoch: 5, holdout mse losses: [0.0061  0.00437 0.00362]\n",
      "epoch: 6, holdout mse losses: [0.00586 0.00551 0.00331]\n",
      "epoch: 7, holdout mse losses: [0.00645 0.00501 0.0032 ]\n",
      "epoch: 8, holdout mse losses: [0.00517 0.0058  0.00339]\n",
      "epoch: 9, holdout mse losses: [0.00484 0.00597 0.0033 ]\n",
      "epoch: 10, holdout mse losses: [0.00474 0.00935 0.0032 ]\n",
      "total step:1300\n",
      "1300 90.66936105489731\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 13\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.00865 0.00367 0.00236]\n",
      "epoch: 1, holdout mse losses: [0.0067  0.00339 0.00287]\n",
      "epoch: 2, holdout mse losses: [0.00417 0.00362 0.10362]\n",
      "epoch: 3, holdout mse losses: [0.00446 0.00273 0.06571]\n",
      "epoch: 4, holdout mse losses: [0.00375 0.00276 0.06629]\n",
      "epoch: 5, holdout mse losses: [0.00309 0.0027  0.04692]\n",
      "epoch: 6, holdout mse losses: [0.00326 0.00274 0.02654]\n",
      "epoch: 7, holdout mse losses: [0.00345 0.00301 0.02116]\n",
      "epoch: 8, holdout mse losses: [0.00271 0.00254 0.01142]\n",
      "epoch: 9, holdout mse losses: [0.00291 0.06274 0.00638]\n",
      "epoch: 10, holdout mse losses: [0.00277 0.0928  0.00471]\n",
      "epoch: 11, holdout mse losses: [0.00359 0.03116 0.00329]\n",
      "epoch: 12, holdout mse losses: [0.01434 0.0187  0.00318]\n",
      "epoch: 13, holdout mse losses: [0.01421 0.0054  0.00279]\n",
      "total step:1400\n",
      "1400 79.53087216615677\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 14\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.02548 0.0081  0.00695]\n",
      "epoch: 1, holdout mse losses: [0.02354 0.00392 0.0049 ]\n",
      "epoch: 2, holdout mse losses: [0.00484 0.00342 0.00389]\n",
      "epoch: 3, holdout mse losses: [0.00562 0.00456 0.00357]\n",
      "epoch: 4, holdout mse losses: [0.00359 0.00369 0.00284]\n",
      "epoch: 5, holdout mse losses: [0.00323 0.00336 0.00294]\n",
      "epoch: 6, holdout mse losses: [0.00305 0.00357 0.00288]\n",
      "epoch: 7, holdout mse losses: [0.00296 0.00329 0.00262]\n",
      "epoch: 8, holdout mse losses: [0.00294 0.00353 0.00358]\n",
      "epoch: 9, holdout mse losses: [0.00286 0.00317 0.00266]\n",
      "epoch: 10, holdout mse losses: [0.0029  0.00333 0.00702]\n",
      "epoch: 11, holdout mse losses: [0.00288 0.0033  0.0204 ]\n",
      "epoch: 12, holdout mse losses: [0.00296 0.00296 0.05303]\n",
      "epoch: 13, holdout mse losses: [0.1153  0.00374 0.0284 ]\n",
      "epoch: 14, holdout mse losses: [0.40401 0.00339 0.01677]\n",
      "epoch: 15, holdout mse losses: [0.0321  0.003   0.00318]\n",
      "epoch: 16, holdout mse losses: [0.01029 0.00703 0.00365]\n",
      "epoch: 17, holdout mse losses: [0.01094 0.00296 0.00298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total step:1500\n",
      "1500 93.90983426570892\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 15\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n",
      "train predict model\n",
      "epoch: 0, holdout mse losses: [0.01083 0.00369 0.00587]\n",
      "epoch: 1, holdout mse losses: [0.00772 0.00355 0.00375]\n",
      "epoch: 2, holdout mse losses: [0.00738 0.007   0.00382]\n",
      "epoch: 3, holdout mse losses: [0.00717 0.00477 0.00346]\n",
      "epoch: 4, holdout mse losses: [0.00502 0.00355 0.00261]\n",
      "epoch: 5, holdout mse losses: [0.00586 0.00282 0.00253]\n",
      "epoch: 6, holdout mse losses: [0.00524 0.00296 0.00247]\n",
      "epoch: 7, holdout mse losses: [0.00442 0.00243 0.00259]\n",
      "epoch: 8, holdout mse losses: [0.00505 0.00691 0.00255]\n",
      "epoch: 9, holdout mse losses: [0.00449 0.03403 0.00424]\n",
      "epoch: 10, holdout mse losses: [0.00502 0.02665 0.01564]\n",
      "epoch: 11, holdout mse losses: [0.0049  0.01159 0.008  ]\n",
      "epoch: 12, holdout mse losses: [0.00498 0.00848 0.00456]\n",
      "total step:1600\n",
      "1600 84.99829983711243\n",
      "i:100\n",
      "cur step:100\n",
      "epoch_step: 16\n",
      "i:0\n",
      "cur step:0\n",
      "i:50\n",
      "cur step:50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c87e93777274>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-a2147b828f33>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0menv_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------------------final training!----------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_sampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-7dbcf785e0ef>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, env_sampler, predict_env, agent, env_pool, model_pool)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_pool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_pool_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mtrain_policy_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_policy_repeats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_policy_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mtotal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7dbcf785e0ef>\u001b[0m in \u001b[0;36mtrain_policy_repeats\u001b[1;34m(args, total_step, train_step, cur_step, env_pool, model_pool, agent)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mbatch_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_done\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mbatch_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mbatch_done\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_next_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_done\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_train_repeat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\JHU\\Work\\Bayesian_RL\\implementation\\DynaPPO\\mbpo_pytorch-main\\sac\\sac.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[1;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mnext_q_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmask_batch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmin_qf_next_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mqf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Two Q-functions to mitigate positive bias in the policy improvement step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mqf1_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_q_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\JHU\\Work\\Bayesian_RL\\implementation\\DynaPPO\\mbpo_pytorch-main\\sac\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuton\\appdata\\local\\programs\\python\\python37\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
