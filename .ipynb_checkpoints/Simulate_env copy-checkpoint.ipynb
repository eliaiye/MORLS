{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d873387f-e729-43fb-b60b-00b0db5e4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm \n",
    "from scipy.stats import lognorm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import mujoco_py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e30f5-2006-4f64-a5ff-2be3fe761d9e",
   "metadata": {},
   "source": [
    "$x_i(x_{i-1}, y, \\Delta t) = a+be^{-c\\Delta t}+d-e(y-y_0)^2$ should satisfy the following conditions:   \n",
    "$x_i = x_{i-1} \\mbox{ when } \\Delta t=0, y=0 \\rightarrow a+b=x_{i-1}$    \n",
    "$x_i = a+be^{-c\\Delta t} \\mbox{ when } y=0 \\rightarrow d=e{y_0}^2$\n",
    "Therefore, $x_i(x_{i-1}, y, \\Delta t)$ can be expressed as $x_{i-1}-b+be^{-c\\Delta t}+e(2yy_0-y^2)$ where we have four parameters: $b, c, e, y_0$           \n",
    "$b$ is the asymptotic change of $x$ without any treatment, higher $b$ means larger change     \n",
    "$c$ is the decrease rate of $x$, higher $c$ means higher rate        \n",
    "$e$ is the sensitivity of $x$ to the dosage, higher $e$ means more sensitive     \n",
    "$y_0$ is the optimal dosage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61563914-34d6-4db5-86b1-5d05f2c5d4f0",
   "metadata": {},
   "source": [
    "# Environment\n",
    "The reward for every step is defined as the surviving days after last visit minus a penalty constant. If the patient did not make it to the next visit, the surviving days is the root of $x_i(\\Delta t)=threshold$.     \n",
    "Note: The root can be negative when the drift part ($e(2yy_0-y^2)$) is large (y deviates too far from $y_0$), but since one cannot live negative days, set surviving days to 0 in this condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "808670a9-bbe8-4ae4-abbd-590663541e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimEnv(gym.Env):\n",
    "    def __init__(self, b=10, c=0.5, e=3, y0=2, threshold=1, penalty=2, timeout=100):\n",
    "        '''\n",
    "        action_space: (y, delta_t)\n",
    "        observation_space: (x)\n",
    "        \n",
    "        Input\n",
    "        threshold: when x is lower than this threshold, patient die\n",
    "        penalty: this parameter adds cost to frequent visits\n",
    "        '''\n",
    "        super(SimEnv, self).__init__()\n",
    "        \n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.e = e\n",
    "        self.y0 = y0\n",
    "        self.threshold = threshold\n",
    "        self.penalty = penalty\n",
    "            \n",
    "        self.action_space = gym.spaces.Box(low=0,high=float(\"inf\"),shape=(2,),dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=-float(\"inf\"), high=float(\"inf\"), shape=(1,), dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.timeout = timeout\n",
    "        self.steps_elapsed = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(self.threshold, 15, 1) # numpy array (1,)\n",
    "        self.steps_elapsed=0\n",
    "        return(self.state)\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        delta_t=action[1]\n",
    "        y=action[0]\n",
    "        next_obs = self.xfunc(x=self.state, delta_t=delta_t, y=action[0], b=self.b,c= self.c, e=self.e, y0=self.y0)\n",
    "        \n",
    "        if next_obs > self.threshold:\n",
    "            reward = delta_t - self.penalty\n",
    "        elif (-1/self.c*np.log((self.threshold-self.state+self.b-self.e*(2*y*self.y0-y**2))/self.b) < 0):\n",
    "            reward = -self.penalty \n",
    "        else:\n",
    "            reward = -1/self.c*np.log((self.threshold-self.state+self.b-self.e*(2*y*self.y0-y**2))/self.b) - self.penalty\n",
    "            \n",
    "        self.state = next_obs \n",
    "        self.steps_elapsed+=1\n",
    "        \n",
    "        return(np.array(self.state).astype(np.float32), reward, next_obs <= self.threshold or self.steps_elapsed > self.timeout , {\"x\": self.state, \"y\": action[0], \"delta_t\": action[1], \"dead\": next_obs <= self.threshold})\n",
    "    \n",
    "    def xfunc(self, x, delta_t, y, b, c, e, y0):\n",
    "        '''\n",
    "        Input\n",
    "        x: last obs/state\n",
    "        delta_t: time interval btw visits\n",
    "        y: dosage\n",
    "        b, c, e, y0: parameters\n",
    "        Output\n",
    "        current obs/state\n",
    "        '''\n",
    "        return(x-b+b*np.exp(-c*delta_t)+e*(2*y*y0-y**2)) \n",
    "    \n",
    "    def render(self, mode=None):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d099dc9-2944-4839-9174-8ee4ded714ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.59627078])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SimEnv()\n",
    "s.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40edbbb9-3433-4571-aa4d-28758483e9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11.275065], dtype=float32),\n",
       " 0,\n",
       " False,\n",
       " {'x': array([11.2750652]), 'y': 1, 'delta_t': 2, 'dead': array([False])})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.step([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b74997a-020c-4d43-b409-fb24d87594de",
   "metadata": {},
   "outputs": [],
   "source": [
    "envgym = gym.make('HalfCheetah-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "383c5732-b0b0-4006-b9d0-af950829fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00476294, -0.04779821, -0.06132714, -0.01496986, -0.05997941,\n",
       "        0.09257857, -0.07259369,  0.05177833, -0.00478871,  0.09428243,\n",
       "        0.17105205, -0.20568064, -0.1102453 , -0.15632871, -0.00046682,\n",
       "        0.03178096,  0.07453983])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envgym.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1cec4c8-cde3-467a-86ea-a305b0a18dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(envgym.step([1,2,3,4,5,6])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5a4a4-2dcf-4fad-87f8-cba1c0dba45f",
   "metadata": {},
   "source": [
    "# Policy - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc45b3e8-3eb0-456c-9c4b-fcaae85991f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "envgym = gym.make('HalfCheetah-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9ce93a-4cd5-475c-8496-46fedf0b7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a14f1e-ad44-4bfe-a286-cf4c33d361e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to smmResults/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -165     |\n",
      "| time/              |          |\n",
      "|    fps             | 942      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 101         |\n",
      "|    ep_rew_mean          | -161        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 256         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009115843 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.000269   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 223         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00904    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 476         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 101         |\n",
      "|    ep_rew_mean          | -158        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 384         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014430724 |\n",
      "|    clip_fraction        | 0.0742      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0066     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 198         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 412         |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 84.7          |\n",
      "|    ep_rew_mean          | -128          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 455           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 512           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085773645 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -0.00225      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 148           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0012       |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 323           |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 59.3       |\n",
      "|    ep_rew_mean          | -87.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 640        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02596501 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.85      |\n",
      "|    explained_variance   | 0.0168     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 152        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 308        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 46.8        |\n",
      "|    ep_rew_mean          | -65.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 768         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019383512 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0329      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.5        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 132         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 42         |\n",
      "|    ep_rew_mean          | -56.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 896        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05178795 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.85      |\n",
      "|    explained_variance   | 0.05       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 67.3       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.032     |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 127        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 24.8       |\n",
      "|    ep_rew_mean          | -31.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 497        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 1024       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03665255 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.84      |\n",
      "|    explained_variance   | 0.00087    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 18.8       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 43         |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy = PPO(\"MlpPolicy\", env, n_steps=128, batch_size=64, verbose=1, tensorboard_log ='smmResults/').learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df70e3b6-9701-4142-8960-e21cacd24469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = []\n",
    "# losses = []\n",
    "# for e in tf.compat.v1.train.summary_iterator('smmResults/PPO_9/PPO1'): # how to automatically change file name\n",
    "#     for v in e.summary.value:\n",
    "#         if v.tag == \"rollout/ep_rew_mean\":\n",
    "#             rewards.append(v.simple_value)\n",
    "#         if v.tag == \"train/loss\":\n",
    "#             losses.append(v.simple_value)\n",
    "# plt.figure()\n",
    "# plt.plot(rewards)\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Episode reward\")\n",
    "# plt.title(\"Mean Episode Reward Over 5 Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a969c1-762d-4e80-b76c-69d5b55f974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = []\n",
    "# length = []\n",
    "# for e in tf.compat.v1.train.summary_iterator('smmResults/PPO_9/PPO1'):\n",
    "#     for v in e.summary.value:\n",
    "#         if v.tag == \"rollout/ep_rew_mean\":\n",
    "#             rewards.append(v.simple_value)\n",
    "#         if v.tag == \"rollout/ep_len_mean\":\n",
    "#             length.append(v.simple_value)\n",
    "# plt.figure()\n",
    "# plt.plot(length)\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Episode Len\")\n",
    "# plt.title(\"Mean Episode Length Over 5 Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd354c-c5e2-4ae4-a99f-b136cc26db47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
