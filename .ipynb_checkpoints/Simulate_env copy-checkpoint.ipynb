{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d873387f-e729-43fb-b60b-00b0db5e4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm \n",
    "from scipy.stats import lognorm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import mujoco_py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e30f5-2006-4f64-a5ff-2be3fe761d9e",
   "metadata": {},
   "source": [
    "$x_i(x_{i-1}, y, \\Delta t) = a+be^{-c\\Delta t}+d-e(y-y_0)^2$ should satisfy the following conditions:   \n",
    "$x_i = x_{i-1} \\mbox{ when } \\Delta t=0, y=0 \\rightarrow a+b=x_{i-1}$    \n",
    "$x_i = a+be^{-c\\Delta t} \\mbox{ when } y=0 \\rightarrow d=e{y_0}^2$\n",
    "Therefore, $x_i(x_{i-1}, y, \\Delta t)$ can be expressed as $x_{i-1}-b+be^{-c\\Delta t}+e(2yy_0-y^2)$ where we have four parameters: $b, c, e, y_0$           \n",
    "$b$ is the asymptotic change of $x$ without any treatment, higher $b$ means larger change     \n",
    "$c$ is the decrease rate of $x$, higher $c$ means higher rate        \n",
    "$e$ is the sensitivity of $x$ to the dosage, higher $e$ means more sensitive     \n",
    "$y_0$ is the optimal dosage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61563914-34d6-4db5-86b1-5d05f2c5d4f0",
   "metadata": {},
   "source": [
    "# Environment\n",
    "The reward for every step is defined as the surviving days after last visit minus a penalty constant. If the patient did not make it to the next visit, the surviving days is the root of $x_i(\\Delta t)=threshold$.     \n",
    "Note: The root can be negative when the drift part ($e(2yy_0-y^2)$) is large (y deviates too far from $y_0$), but since one cannot live negative days, set surviving days to 0 in this condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808670a9-bbe8-4ae4-abbd-590663541e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimEnv(gym.Env):\n",
    "    def __init__(self, b=10, c=0.5, e=3, y0=2, threshold=1, penalty=2, timeout=100):\n",
    "        '''\n",
    "        action_space: (y, delta_t)\n",
    "        observation_space: (x)\n",
    "        \n",
    "        Input\n",
    "        threshold: when x is lower than this threshold, patient die\n",
    "        penalty: this parameter adds cost to frequent visits\n",
    "        '''\n",
    "        super(SimEnv, self).__init__()\n",
    "        \n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.e = e\n",
    "        self.y0 = y0\n",
    "        self.threshold = threshold\n",
    "        self.penalty = penalty\n",
    "            \n",
    "        self.action_space = gym.spaces.Box(low=0,high=float(\"inf\"),shape=(2,),dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=-float(\"inf\"), high=float(\"inf\"), shape=(1,), dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.timeout = timeout\n",
    "        self.steps_elapsed = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(self.threshold, 15, 1) # numpy array (1,)\n",
    "        self.steps_elapsed=0\n",
    "        return(self.state)\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        delta_t=action[1]\n",
    "        y=action[0]\n",
    "        next_obs = self.xfunc(x=self.state, delta_t=delta_t, y=action[0], b=self.b,c= self.c, e=self.e, y0=self.y0)\n",
    "        \n",
    "        if next_obs > self.threshold:\n",
    "            reward = delta_t - self.penalty\n",
    "        elif (-1/self.c*np.log((self.threshold-self.state[0]+self.b-self.e*(2*y*self.y0-y**2))/self.b) < 0):\n",
    "            reward = -self.penalty \n",
    "        else:\n",
    "            reward = -1/self.c*np.log((self.threshold-self.state[0]+self.b-self.e*(2*y*self.y0-y**2))/self.b) - self.penalty\n",
    "            \n",
    "        self.state = next_obs \n",
    "        self.steps_elapsed+=1\n",
    "        \n",
    "        return(np.array([self.state]).astype(np.float32), reward, next_obs[0] <= self.threshold or self.steps_elapsed > self.timeout , {\"x\": self.state, \"y\": action[0], \"delta_t\": action[1], \"dead\": next_obs[0] <= self.threshold})\n",
    "    \n",
    "    def xfunc(self, x, delta_t, y, b, c, e, y0):\n",
    "        '''\n",
    "        Input\n",
    "        x: last obs/state\n",
    "        delta_t: time interval btw visits\n",
    "        y: dosage\n",
    "        b, c, e, y0: parameters\n",
    "        Output\n",
    "        current obs/state\n",
    "        '''\n",
    "        return(x-b+b*np.exp(-c*delta_t)+e*(2*y*y0-y**2)) \n",
    "    \n",
    "    def render(self, mode=None):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d099dc9-2944-4839-9174-8ee4ded714ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.02582458])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SimEnv()\n",
    "s.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40edbbb9-3433-4571-aa4d-28758483e9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s.step([1,2])[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5a4a4-2dcf-4fad-87f8-cba1c0dba45f",
   "metadata": {},
   "source": [
    "# Policy - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc45b3e8-3eb0-456c-9c4b-fcaae85991f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "envgym = gym.make('HalfCheetah-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9ce93a-4cd5-475c-8496-46fedf0b7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a14f1e-ad44-4bfe-a286-cf4c33d361e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to smmResults/PPO_34\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | -38.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 793      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.2        |\n",
      "|    ep_rew_mean          | -56.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 654         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 256         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022998236 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.00762     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 201         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 442         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 46.3        |\n",
      "|    ep_rew_mean          | -70.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 384         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007838302 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 362         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.3        |\n",
      "|    ep_rew_mean          | -70.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 495         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 512         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020985447 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.0242     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 351         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 52.5        |\n",
      "|    ep_rew_mean          | -74.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 538         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 640         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022347622 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.0221     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 259         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.6        |\n",
      "|    ep_rew_mean          | -65.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 557         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 768         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023862932 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.00842     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 105         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 245         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.3        |\n",
      "|    ep_rew_mean          | -36.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 562         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 896         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023367543 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.00346    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 66          |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 159         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 17.8       |\n",
      "|    ep_rew_mean          | -23.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 515        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 1024       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09644764 |\n",
      "|    clip_fraction        | 0.493      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.85      |\n",
      "|    explained_variance   | 0.0531     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 18.2       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0567    |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 41.1       |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy = PPO(\"MlpPolicy\", env, n_steps=128, batch_size=64, verbose=1, tensorboard_log ='smmResults/').learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df70e3b6-9701-4142-8960-e21cacd24469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = []\n",
    "# losses = []\n",
    "# for e in tf.compat.v1.train.summary_iterator('smmResults/PPO_9/PPO1'): # how to automatically change file name\n",
    "#     for v in e.summary.value:\n",
    "#         if v.tag == \"rollout/ep_rew_mean\":\n",
    "#             rewards.append(v.simple_value)\n",
    "#         if v.tag == \"train/loss\":\n",
    "#             losses.append(v.simple_value)\n",
    "# plt.figure()\n",
    "# plt.plot(rewards)\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Episode reward\")\n",
    "# plt.title(\"Mean Episode Reward Over 5 Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a969c1-762d-4e80-b76c-69d5b55f974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = []\n",
    "# length = []\n",
    "# for e in tf.compat.v1.train.summary_iterator('smmResults/PPO_9/PPO1'):\n",
    "#     for v in e.summary.value:\n",
    "#         if v.tag == \"rollout/ep_rew_mean\":\n",
    "#             rewards.append(v.simple_value)\n",
    "#         if v.tag == \"rollout/ep_len_mean\":\n",
    "#             length.append(v.simple_value)\n",
    "# plt.figure()\n",
    "# plt.plot(length)\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Episode Len\")\n",
    "# plt.title(\"Mean Episode Length Over 5 Episodes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
