{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9c996157-6ada-4abc-ac96-54a2af63a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f7df59c2-9843-48a4-b860-ed0a0c0368ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimEnv(gym.Env):\n",
    "    def __init__(self, b=10, c=0.5, e=3, y0=2, threshold=1, penalty=2, timeout=100):\n",
    "        '''\n",
    "        action_space: (y, delta_t)\n",
    "        observation_space: (x)\n",
    "        \n",
    "        Input\n",
    "        threshold: when x is lower than this threshold, patient die\n",
    "        penalty: this parameter adds cost to frequent visits\n",
    "        '''\n",
    "        super(SimEnv, self).__init__()\n",
    "        \n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.e = e\n",
    "        self.y0 = y0\n",
    "        self.threshold = threshold\n",
    "        self.penalty = penalty\n",
    "            \n",
    "        self.action_space = gym.spaces.Box(low=0,high=float(\"inf\"),shape=(2,),dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=-float(\"inf\"), high=float(\"inf\"), shape=(1,), dtype=np.float32)\n",
    "        self.state = None\n",
    "        self.timeout = timeout\n",
    "        self.steps_elapsed = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(self.threshold, 15, 1) # numpy array (1,)\n",
    "        self.steps_elapsed=0\n",
    "        return(self.state)\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        delta_t=action[1]\n",
    "        y=action[0]\n",
    "        next_obs = self.xfunc(x=self.state, delta_t=delta_t, y=action[0], b=self.b,c= self.c, e=self.e, y0=self.y0)\n",
    "        \n",
    "        if next_obs > self.threshold:\n",
    "            reward = delta_t - self.penalty\n",
    "        elif (-1/self.c*np.log((self.threshold-self.state[0]+self.b-self.e*(2*y*self.y0-y**2))/self.b) < 0):\n",
    "            reward = -self.penalty \n",
    "        else:\n",
    "            reward = -1/self.c*np.log((self.threshold-self.state[0]+self.b-self.e*(2*y*self.y0-y**2))/self.b) - self.penalty\n",
    "            \n",
    "        self.state = next_obs \n",
    "        self.steps_elapsed+=1\n",
    "        \n",
    "        return(np.array([self.state]).astype(np.float32), reward, next_obs[0] <= self.threshold or self.steps_elapsed > self.timeout , {\"x\": self.state, \"y\": action[0], \"delta_t\": action[1], \"dead\": next_obs <= self.threshold})\n",
    "    \n",
    "    def xfunc(self, x, delta_t, y, b, c, e, y0):\n",
    "        '''\n",
    "        Input\n",
    "        x: last obs/state\n",
    "        delta_t: time interval btw visits\n",
    "        y: dosage\n",
    "        b, c, e, y0: parameters\n",
    "        Output\n",
    "        current obs/state\n",
    "        '''\n",
    "        return(x-b+b*np.exp(-c*delta_t)+e*(2*y*y0-y**2)) \n",
    "    \n",
    "    def render(self, mode=None):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7722077-bd93-4bae-8cd0-9ea327b62c4c",
   "metadata": {},
   "source": [
    "Actor Critic Policy:\n",
    "- Two-layer MLP, 64 neurons, activation tanh, output Gaussian mean\n",
    "- Log standard deviation (same dimension as action)\n",
    "- Policy: $\\mathcal{N}(\\mbox{mean}=\\mbox{NeuralNet}(s;{W_i, b_i}^L_{i=1}), \\mbox{stdev}=e^r)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "96843a09-9b19-4841-996f-69033ea7cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_neurons=64, activation=nn.Tanh, distribution = torch.distributions.multivariate_normal.MultivariateNormal):\n",
    "        '''\n",
    "        input dim = obs dim\n",
    "        output dim = action dim\n",
    "        '''\n",
    "        super(ActorCriticPolicy, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_neurons = n_neurons\n",
    "        self.distribution = distribution\n",
    "\n",
    "        # Policy Network\n",
    "        self.h0 = nn.Linear(input_dim, n_neurons)\n",
    "        self.h0_act = activation()\n",
    "        self.h1 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.h1_act = activation()\n",
    "        self.output_layer = nn.Linear(n_neurons, output_dim)\n",
    "\n",
    "        # Value Network\n",
    "        self.h0v = nn.Linear(input_dim, n_neurons)\n",
    "        self.h0_actv = activation()\n",
    "        self.h1v = nn.Linear(n_neurons, n_neurons)\n",
    "        self.h1_actv = activation()\n",
    "        self.value_head = nn.Linear(n_neurons, 1)\n",
    "\n",
    "        self.variance = torch.nn.Parameter(torch.tensor([0.0]*self.output_dim), requires_grad = True)\n",
    "\n",
    "        #self.mean_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, obs, action=None):\n",
    "        '''\n",
    "        obs: tensor.size([input_dim])\n",
    "        '''\n",
    "        # Policy Forward: obs->action\n",
    "        x = self.h0(obs)\n",
    "        x = self.h0_act(x)\n",
    "        x = self.h1(x)\n",
    "        x = self.h1_act(x)\n",
    "        action_mean = self.output_layer(x) #tensor.size([#obs,act_dim])\n",
    "\n",
    "        action_variance = torch.exp(self.variance)\n",
    "        action_dist = self.distribution(action_mean, torch.diag_embed(action_variance)) #tensor.size([#obs, obs_dim, obs_dim])\n",
    "\n",
    "        if action is None:\n",
    "            action = action_dist.sample() #tensor.size([#obs, act_dim])\n",
    "        \n",
    "        logprob = action_dist.log_prob(action) #tensor.size([#obs])\n",
    "        \n",
    "        # Value Forward: obs-> state value(V)\n",
    "        x = self.h0v(obs)\n",
    "        x = self.h0_actv(x)\n",
    "        x = self.h1v(x)\n",
    "        x = self.h1_actv(x)\n",
    "        value = self.value_head(x)\n",
    "        value = torch.squeeze(value) #tensor.size([#obs,1]) - > tensor.size([#obs])\n",
    "\n",
    "        return action, logprob, value # action: tensor.size([#obs, act_dim]), logprob: tensor.size([#obs]), value: tensor.size([#obs])\n",
    "        # Question: Do we need the entropy of the distribution?\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "00807b30-0714-4a70-ba9b-5b4e153573b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_X():\n",
    "    def __init__(self, input_dim, output_dim, network=ActorCriticPolicy):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.policy = network(input_dim, output_dim)\n",
    "        \n",
    "        self.total_obs = []\n",
    "        self.total_act = []\n",
    "        self.total_rew = []\n",
    "        self.total_ep_rew=[]\n",
    "        self.total_ep_len=[]\n",
    "        \n",
    "    def forward(self, observation, action = None):\n",
    "        return self.policy.forward(observation, action=action)\n",
    "    \n",
    "    def rollout(self, env, timesteps, gamma, lamb): # timesteps << episode length\n",
    "        '''\n",
    "        Run policy pi_old in environment for T timesteps\n",
    "        Compute advantage estimates A_1,...,A_T\n",
    "        '''\n",
    "        obs_trajectory = []\n",
    "        action_trajectory = []\n",
    "        logprob_trajectory = []\n",
    "        value_trajectory = []\n",
    "        reward_trajectory = []\n",
    "        done_trajectory = []\n",
    "        \n",
    "        done = True\n",
    "        for i in range(timesteps):\n",
    "            if done:\n",
    "                obs = torch.unsqueeze(torch.tensor(env.reset()).float(),0) # numpy array shape (obs_dim, ) \n",
    "                                                                   # -> tensor.size([obs_dim])\n",
    "                                                                   # -> tensor.size([1, obs_dim])\n",
    "                done = False\n",
    "            \n",
    "            action, logprob, value = self.forward(observation = obs) # action: tensor.size([1, act_dim]), logprob: tensor.size([1]), value: tensor.size([1])\n",
    "            \n",
    "            action = torch.squeeze(action) # tensor([act_dim])\n",
    "            logprob = torch.squeeze(logprob) # tensor(1)\n",
    "            value = torch.squeeze(value) # tensor(1)\n",
    "            \n",
    "            action_trajectory.append(action)\n",
    "            logprob_trajectory.append(logprob)\n",
    "            value_trajectory.append(value)\n",
    "            obs_trajectory.append(torch.squeeze(obs))\n",
    "            done_trajectory.append(done)\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action.numpy().astype(float)) # step() takes np array shape(act_dim,)\n",
    "            obs = torch.unsqueeze(torch.tensor(obs).float(),0)\n",
    "            reward = torch.tensor(reward).float() # tensor.size([])\n",
    "            #print(\"reward\", reward, \"done\", done)\n",
    "            \n",
    "            reward_trajectory.append(reward)\n",
    "        # All trajectories are lists of squeezed tensors, done_trajectory is a list of bool\n",
    "        # List->stacked tensor\n",
    "        obs_trajectory = torch.stack(obs_trajectory) # tensor.size([#obs, obs_dim])\n",
    "        action_trajectory = torch.stack(action_trajectory)\n",
    "        logprob_trajectory = torch.stack(logprob_trajectory)# tensor.size([#obs])\n",
    "        value_trajectory = torch.stack(value_trajectory)\n",
    "        reward_trajectory = torch.stack(reward_trajectory)  \n",
    "        \n",
    "        done_trajectory = np.asarray(done_trajectory, dtype=np.bool_)\n",
    "        \n",
    "        _, __, last_value = self.forward(obs)\n",
    "        \n",
    "        adv_trajectory = torch.zeros_like(reward_trajectory).float()\n",
    "        delta_trajectory = torch.zeros_like(reward_trajectory).float()\n",
    "        \n",
    "        for t in reversed(range(timesteps)): #T-1 -> 0\n",
    "            \n",
    "            if done_trajectory[t]:\n",
    "                delta_trajectory[t] = reward_trajectory[t]-value_trajectory[t]\n",
    "                adv_trajectory[t]=delta_trajectory[t]\n",
    "            \n",
    "            else:\n",
    "                if t == timesteps-1:\n",
    "                    delta_trajectory[t] = reward_trajectory[t]-value_trajectory[t]\n",
    "                    adv_trajectory[t]=delta_trajectory[t]\n",
    "                else:\n",
    "                    delta_trajectory[t] = reward_trajectory[t]+gamma*value_trajectory[t+1]-value_trajectory[t]\n",
    "                    adv_trajectory[t]=delta_trajectory[t]+gamma*lamb*adv_trajectory[t+1]\n",
    "            \n",
    "#             if t == timesteps-1:\n",
    "#                 adv_trajectory[t]=delta_trajectory[t]\n",
    "                \n",
    "                \n",
    "#             else:\n",
    "#                 adv_trajectory[t]=delta_trajectory[t]+gamma*lamb*adv_trajectory[t+1] #recursion\n",
    "        #print(\"in rollout:\",obs_trajectory.size())\n",
    "        return(obs_trajectory, action_trajectory, logprob_trajectory, adv_trajectory)\n",
    "    \n",
    "    def loss(self, epsilon, obs_traj, action_traj, logprob_traj, adv_traj, current_logprob):\n",
    "        # ratio\n",
    "        ratio = torch.exp(current_logprob-logprob_traj) # torch.size([#obs])\n",
    "        \n",
    "        # loss\n",
    "        l1 = ratio*adv_traj # torch.size([#obs])\n",
    "        l2 = torch.clamp(ratio, 1.-epsilon, 1.+epsilon) # torch.size([#obs])\n",
    "        # Minimize negative L\n",
    "        PGloss = -torch.mean(torch.min(l1, l2))# torch.size([#obs])\n",
    "            \n",
    "        return PGloss # torch.size([])\n",
    "    \n",
    "    def train_step(self, epsilon, obs_traj, action_traj, logprob_traj, adv_traj):\n",
    "        \n",
    "        #Question: adv needs normalization?\n",
    "        \n",
    "        self.policy.train() # module train mode\n",
    "        with torch.set_grad_enabled(True):\n",
    "            action, logprob, value = self.forward(obs_traj, action=action_traj)\n",
    "            \n",
    "            PGloss = self.loss(epsilon, obs_traj, action_traj, logprob_traj, adv_traj,logprob)\n",
    "            \n",
    "            PGloss.backward()\n",
    "            \n",
    "            return(PGloss)\n",
    "    \n",
    "    def train(self, env, total_timesteps, timesteps=1024, k_epoch=4, num_batch=4, n_actor=1, \n",
    "              epsilon=0.2, gamma = 0.99, lamb=0.95, lr = 0.00027, optimizer = torch.optim.Adam, evaluate=True):\n",
    "        \n",
    "        n_updates = total_timesteps//timesteps # number of iterations\n",
    "        \n",
    "        self.policy_optimizer=optimizer(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        for i in tqdm(range(n_updates)):\n",
    "            actor=0\n",
    "            obs_trajectory, action_trajectory, logprob_trajectory, adv_trajectory = self.rollout(env, timesteps, gamma, lamb)\n",
    "            for actor in range(n_actor-1):\n",
    "                more_obs_traj, more_action_traj, more_logprob_traj, more_adv_traj = self.rollout(env, timesteps, gamma, lamb)\n",
    "                obs_trajectory = torch.cat((obs_trajectory, more_obs_traj),0)\n",
    "                action_trajectory = torch.cat((action_trajectory, more_action_traj),0)\n",
    "                logprob_trajectory = torch.cat((logprob_trajectory, more_logprob_traj),0)\n",
    "                adv_trajectory = torch.cat((adv_trajectory, more_adv_traj),0)\n",
    "            \n",
    "            indices = np.arange(n_actor*timesteps) # [0, 1, ..., n_actor*timesteps-1]\n",
    "            \n",
    "            for k in range(k_epoch):\n",
    "                np.random.shuffle(indices)\n",
    "                batch_size = n_actor*timesteps//num_batch\n",
    "                \n",
    "                if(timesteps%num_batch):\n",
    "                    batch_size += 1\n",
    "                \n",
    "                for b in range(num_batch):\n",
    "                    #reset gradient\n",
    "                    self.policy.zero_grad()\n",
    "                    \n",
    "                    if b != num_batch-1: # all batches except the last one\n",
    "                        batch_indices = indices[b*batch_size:(b+1)*batch_size]\n",
    "                    else:\n",
    "                        batch_indices = indices[b*batch_size:] # last batch\n",
    "                \n",
    "                    batch=[tensor[batch_indices] for tensor in (torch.unsqueeze(obs_trajectory,1), action_trajectory, logprob_trajectory, adv_trajectory)]\n",
    "                    #print(\"in train\", batch[0])\n",
    "                    PGloss = self.train_step(epsilon, *batch) \n",
    "                    \n",
    "                    #Question: Do we need to clip gradient?\n",
    "                    \n",
    "                    self.policy_optimizer.step()\n",
    "            if evaluate:\n",
    "                eval_policy(env)\n",
    "                    \n",
    "        def eval_policy(self, env, n_ep=5, timeout=None):\n",
    "            if timeout==None:\n",
    "                timeout = env.timeout\n",
    "                \n",
    "            for i_episode in range(n_ep):\n",
    "                obs = []\n",
    "                act = []\n",
    "                rew = []\n",
    "                ep_reward_ls = []\n",
    "                ep_len_ls=[]\n",
    "                ep_reward = 0\n",
    "                ep_len=0\n",
    "                observation = self.env.reset()\n",
    "                obs.append(observation)\n",
    "                for t in range(timeout):\n",
    "                    action = self.act_sample()\n",
    "                    observation, reward, done, info = self.env.step(action)\n",
    "                    #print(\"reward:\", reward)\n",
    "                    ep_reward += reward\n",
    "                    ep_len +=1\n",
    "                    rew.append(reward)\n",
    "                    act.append(action)\n",
    "                    obs.append(observation)\n",
    "\n",
    "                    if done or t == 99:\n",
    "                        #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                        #print(\"Survival time: {}\".format(ep_reward+env.penalty*(t+1)))\n",
    "                        break\n",
    "                        \n",
    "                ep_reward_ls.append(ep_reward) \n",
    "                ep_len_ls.append(ep_len)\n",
    "                self.total_rew.append(rew)\n",
    "                self.total_act.append(act)\n",
    "                self.total_obs.append(obs)\n",
    "\n",
    "            self.total_ep_rew.append(np.mean(ep_reward))\n",
    "            self.total_ep_len.append(np.mean(ep_len))\n",
    "         \n",
    "    def eval(self, obs):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            obs = torch.unsqueeze(torch.tensor(obs).float(),0)\n",
    "            action, _, __=self.forward(obs)\n",
    "        return(torch.squeeze(action))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "cf671272-3524-4dc2-8652-67cc837f2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env =SimEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "67398712-93e0-40b9-acfb-8cb7b72ea466",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=PPO_X(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8e0ad028-2a98-4ad6-bf67-5766282d0999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-79a99988a489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 100 iterations, 1 actor, 4 epochs, batch size=100/4=25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-330-a971f9a465ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, total_timesteps, timesteps, k_epoch, num_batch, n_actor, epsilon, gamma, lamb, lr, optimizer, evaluate)\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_trajectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_trajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob_trajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_trajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;31m#print(\"in train\", batch[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mPGloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0;31m#Question: Do we need to clip gradient?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-330-a971f9a465ee>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, epsilon, obs_traj, action_traj, logprob_traj, adv_traj)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mPGloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_traj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mPGloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPGloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "policy.train(env=env, total_timesteps=10000, timesteps=100) # 100 iterations, 1 actor, 4 epochs, batch size=100/4=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ae216-c17f-415e-82f8-813ae6d12399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
